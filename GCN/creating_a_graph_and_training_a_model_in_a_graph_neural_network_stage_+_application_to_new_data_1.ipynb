{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# required libraries"
      ],
      "metadata": {
        "id": "9ijwj5RhH1CR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import geopandas as gpd\n",
        "import pandas as pd\n",
        "from shapely.geometry import Point\n",
        "import matplotlib.pyplot as plt\n",
        "from geopy.distance import distance\n",
        "import plotly.graph_objects as go\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score"
      ],
      "metadata": {
        "id": "cNqJqcBEHx0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 1 - education predict"
      ],
      "metadata": {
        "id": "E7NtXnoqGmD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "code to create graph"
      ],
      "metadata": {
        "id": "EZukJcAXEK_k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oxm4-yCfDWR_"
      },
      "outputs": [],
      "source": [
        "buffer_size = 300\n",
        "n_closest = 3\n",
        "path_root_city = Path('/content')\n",
        "filename = 'data_high1.csv'\n",
        "filepath = path_root_city / filename\n",
        "filepath_subway = path_root_city / 'метро.geojson'\n",
        "filepath_bus = path_root_city / 'остановки автобусов.geojson'\n",
        "filepath_tram = path_root_city / 'остановки трамвай.geojson'\n",
        "df_squares = gpd.read_file(path_root_city / 'squares1.csv')\n",
        "df_squares['building_id'] = df_squares['building_id'].astype(int)\n",
        "\n",
        "def read_geojson(path: Path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def extract_stops(path: Path) -> list[tuple[int, float, float]]:\n",
        "    data = read_geojson(path)\n",
        "    coords = list(map(lambda v: (int(v['properties']['osm_id']),\n",
        "                                 v['geometry']['coordinates'][0],\n",
        "                                 v['geometry']['coordinates'][1]), data['features']))\n",
        "    return coords\n",
        "\n",
        "def draw_plotly(g_):\n",
        "    import plotly.express as px\n",
        "    pos_ = nx.get_node_attributes(g_, 'coordinate')\n",
        "    nodes = list(g_.nodes())\n",
        "    edge_x = []\n",
        "    edge_y = []\n",
        "    edges_text = []\n",
        "    for edge in g_.edges(data=True):\n",
        "        x0, y0 = pos_[edge[0]]\n",
        "        x1, y1 = pos_[edge[1]]\n",
        "        edge_x.extend([x0, x1, None])\n",
        "        edge_y.extend([y0, y1, None])\n",
        "        edges_text.extend([f\"Meters: {edge[2]['meters']}', f'Meters: {edge[2]['meters']}\", None])\n",
        "    nodes_data = g_.nodes(data=True)\n",
        "    node_x = []\n",
        "    node_y = []\n",
        "    node_text = []\n",
        "    node_colors = []\n",
        "    for node in nodes:\n",
        "        x, y = pos_[node]\n",
        "        node_x.append(x)\n",
        "        node_y.append(y)\n",
        "        attrs = nodes_data[node]\n",
        "        if attrs['label'] in {'subway', 'bus', 'tram'}:\n",
        "            node_colors.append('black')\n",
        "        else:\n",
        "            node_colors.append('green' if attrs['education'] > 0 else 'red')\n",
        "        attrs = dict(filter(lambda kv: kv[0] not in {'coordinate', 'label'}, attrs.items()))\n",
        "        node_text.append('<br>'.join(list(map(lambda kv: f'{kv[0]}: {kv[1]}', attrs.items()))))\n",
        "    fig = go.Figure()\n",
        "    df_nodes = pd.DataFrame.from_records(list(dict(nodes_data).values())).drop(columns='coordinate')\n",
        "    df_nodes['s_cnt'] = df_nodes[['kindergarten', 'bus_buf', 'school', 'tram_stop', 'subway_buf']].sum(axis=1)\n",
        "    df_nodes['color'] = 'gray'\n",
        "    df_nodes.loc[df_nodes['s_cnt'] > 0, 'color'] = 'blue'\n",
        "    df_nodes.loc[df_nodes['education'] == 1, 'color'] = 'green'\n",
        "    df_nodes.loc[df_nodes['label'].isin({'tram', 'subway', 'bus'}), 'color'] = 'black'\n",
        "    fig = px.scatter_map(df_nodes, lat=\"lon\", lon=\"lat\", hover_name=\"education\", hover_data=[\"label\", \"square\"],\n",
        "                         color=df_nodes['color'],\n",
        "                         zoom=8,\n",
        "                         height=1000)\n",
        "    fig.update_layout(map_style=\"open-street-map\")\n",
        "    fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
        "    fig.update_layout(\n",
        "        title='Interactive Graph with NetworkX and Plotly',\n",
        "        hovermode='closest',\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "def calculate_distances(geometry: gpd.GeoSeries, point: tuple[float, float]) -> pd.Series:\n",
        "    dst = gpd.GeoSeries([Point(point), ], crs='epsg:4326').to_crs(epsg=3857)\n",
        "    return geometry.to_crs(epsg=3857).distance(dst[0])\n",
        "\n",
        "def add_stops(\n",
        "        g_: nx.Graph,\n",
        "        positions: dict[int, tuple[float, float]],\n",
        "        stops: list[tuple[int, float, float]],\n",
        "        label: str\n",
        "):\n",
        "    geometry = [Point(xy) for xy in list(positions.values())]\n",
        "    gdf_pos = gpd.GeoDataFrame(list(zip(positions.keys(), positions.values())), columns=['id', 'geometry'], crs='epsg:4326', geometry=geometry)\n",
        "    for stop_id, lat, lon in stops:\n",
        "        distances = calculate_distances(gdf_pos.geometry, (lat, lon))\n",
        "        distances_less = distances[distances < buffer_size]\n",
        "        if distances_less.shape[0] > 0:\n",
        "            if g_.has_node(stop_id):\n",
        "                raise ValueError('The node already exists')\n",
        "            g_.add_node(stop_id, coordinate=(lat, lon),\n",
        "                        label=label, education=0, kindergarten=0, school=0, bus_buf=0, tram_stop=0, subway_buf=0,\n",
        "                        lat=lat, lon=lon)\n",
        "            for building_id in gdf_pos[distances < buffer_size]['id']:\n",
        "                g_.add_edge(building_id, stop_id, meters=0)\n",
        "\n",
        "def build_data():\n",
        "    cache_file_path = Path('/content') / filename.replace('.csv', '.pkl')\n",
        "    if cache_file_path.exists():\n",
        "        with open(cache_file_path, 'rb') as f:\n",
        "            graph = pickle.load(f)\n",
        "        return graph\n",
        "    df = pd.read_csv(filepath)\n",
        "    data_json = {'features': df.to_dict(orient='records')}\n",
        "    stops_bus = extract_stops(filepath_bus)\n",
        "    stops_tram = extract_stops(filepath_tram)\n",
        "    stops_sub = extract_stops(filepath_subway)\n",
        "    graph = nx.Graph()\n",
        "    squares_map = df_squares.set_index('building_id')['building_area'].to_dict()\n",
        "    n_count = len(data_json['features'])\n",
        "    for i, building in enumerate(data_json['features']):\n",
        "        if i % 100 == 0:\n",
        "            print(f'Calc {i} / {n_count}')\n",
        "        props = building['properties'] if 'properties' in building else building\n",
        "        building_id = props['building_id']\n",
        "        square = float(squares_map[building_id])\n",
        "        education = int(props['education'] > 0)\n",
        "        if square < 200:\n",
        "            print(f'Skip building \"{building_id}\" with square {square} and education={education}')\n",
        "            continue\n",
        "        x_source, y_source = building['x'], building['y']\n",
        "        graph.add_node(props['building_id'], coordinate=(x_source, y_source),\n",
        "                       square=square,\n",
        "                       label='Y' if props['education'] else 'N',\n",
        "                       kindergarten=props['kindergarten'],\n",
        "                       school =props['school'],\n",
        "                       bus_buf=props['bus_buf'],\n",
        "                       tram_stop=props['tram_stop'],\n",
        "                       subway_buf=props['subway_buf'],\n",
        "                       education=education,\n",
        "                       lat=x_source,\n",
        "                       lon=y_source\n",
        "                       )\n",
        "    print('The nodes was build')\n",
        "    pos_: dict[int, tuple[float, float]] = nx.get_node_attributes(graph, 'coordinate')\n",
        "    geometry = [Point(xy) for xy in list(pos_.values())]\n",
        "    gdf_pos = gpd.GeoDataFrame(list(zip(pos_.keys(), pos_.values())), columns=['id', 'geometry'], crs='epsg:4326', geometry=geometry)\n",
        "    n_count = len(pos_)\n",
        "    for i, (building_id, coord) in enumerate(pos_.items()):\n",
        "        if i % 100 == 0:\n",
        "            print(f'Calc {i} / {n_count}')\n",
        "        gdf_pos['distances'] = calculate_distances(gdf_pos.geometry, coord)\n",
        "        indices = (gdf_pos['distances'] > 0) & (gdf_pos['distances'] < buffer_size)\n",
        "        if indices.any():\n",
        "            gdf_close = gdf_pos[gdf_pos['distances'] < buffer_size]\n",
        "        else:\n",
        "            gdf_close = gdf_pos.sort_values('distances')[1:n_closest]\n",
        "        was_added_any = False\n",
        "        for next_building_id, meters in zip(gdf_close['id'], gdf_close['distances']):\n",
        "            if next_building_id == building_id:\n",
        "                continue\n",
        "            graph.add_edge(building_id, next_building_id, meters=meters)\n",
        "            was_added_any = True\n",
        "        if not was_added_any:\n",
        "            raise ValueError('...')\n",
        "    print('The graph was build')\n",
        "    add_stops(graph, pos_, stops_bus, 'bus')\n",
        "    print('The bus were added')\n",
        "    add_stops(graph, pos_, stops_tram, 'tram')\n",
        "    print('The tram were added')\n",
        "    add_stops(graph, pos_, stops_sub, 'subway')\n",
        "    print('The subway were added')\n",
        "    with open(cache_file_path, 'wb') as f:\n",
        "        pickle.dump(graph, f)\n",
        "    return graph\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    graph_data = build_data()\n",
        "    draw_plotly(graph_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "binary classification of graph using graph neural network GCN"
      ],
      "metadata": {
        "id": "pk5FLG7WEPFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'data_high1.pkl'\n",
        "\n",
        "def read_data() -> nx.Graph:\n",
        "    cache_file_path = Path('/content') / filename\n",
        "    with open(cache_file_path, 'rb') as f:\n",
        "        graph = pickle.load(f)\n",
        "    return graph\n",
        "\n",
        "G: nx.Graph = read_data()\n",
        "N_FEATURES = 9\n",
        "\n",
        "features = []\n",
        "labels = []\n",
        "node_mapping = {}\n",
        "for i, (node_id, node_data) in enumerate(G.nodes(data=True)):\n",
        "    node_mapping[node_id] = i\n",
        "    is_stop = 0\n",
        "    if node_data['label'] == 'bus':\n",
        "        is_stop = 1\n",
        "    elif node_data['label'] == 'tram':\n",
        "        is_stop = 2\n",
        "    elif node_data['label'] == 'subway':\n",
        "        is_stop = 3\n",
        "    features.append([\n",
        "        node_data.get('square', 1),\n",
        "        node_data.get('building_area', 1),\n",
        "        node_data.get('living_area', 1),\n",
        "        node_data.get('population_balanced', 1),\n",
        "        node_data['school'],\n",
        "        node_data['kindergarten'],\n",
        "        node_data['bus_buf'],\n",
        "        node_data['tram_stop'],\n",
        "        node_data['subway_buf'],\n",
        "    ])\n",
        "    labels.append(int(node_data['education'] > 0))\n",
        "\n",
        "features = torch.tensor(features, dtype=torch.float)\n",
        "num_nodes = features.size(0)\n",
        "labels = torch.tensor(labels, dtype=torch.long)\n",
        "train_indices, test_indices = train_test_split(\n",
        "    np.arange(num_nodes),\n",
        "    test_size=0.3,\n",
        "    stratify=labels.numpy(),\n",
        "    random_state=21\n",
        ")\n",
        "edges = list(map(lambda v: (node_mapping[v[0]], node_mapping[v[1]]), G.edges))\n",
        "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "data = Data(x=features, edge_index=edge_index, y=labels,\n",
        "            train_mask=train_indices, test_mask=test_indices)\n",
        "assert edge_index.max() < num_nodes, \"Edge index is out of bounds!\"\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(N_FEATURES, 32)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(32)\n",
        "        self.conv2 = GCNConv(32, 64)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(64)\n",
        "        self.conv3 = GCNConv(64, 128)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(128)\n",
        "        self.conv4 = GCNConv(128, 256)\n",
        "        self.bn4 = torch.nn.BatchNorm1d(256)\n",
        "        self.conv5 = GCNConv(256, 512)\n",
        "        self.bn5 = torch.nn.BatchNorm1d(512)\n",
        "        self.conv6 = GCNConv(512, 1024)\n",
        "        self.bn6 = torch.nn.BatchNorm1d(1024)\n",
        "        self.conv7 = GCNConv(1024, 2)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv4(x, edge_index)\n",
        "        x = self.bn4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv5(x, edge_index)\n",
        "        x = self.bn5(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv6(x, edge_index)\n",
        "        x = self.bn6(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv7(x, edge_index)\n",
        "        return x\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "model = GCN().to(device)\n",
        "data = data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "class_weights = torch.tensor([1., 10.])\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "def calc_scores(out_, target_) -> tuple[float, float, float]:\n",
        "    out_max = out_.max(1)[1]\n",
        "    correct = target_.eq(out_max).sum().item()\n",
        "    acc = correct / target_.size()[0]\n",
        "    correct1 = target_[target_.eq(1)].eq(out_max[target_.eq(1)]).sum().item()\n",
        "    acc1 = correct1 / target_[target_.eq(1)].size()[0]\n",
        "    f1 = float(f1_score(target_.numpy(), out_max.numpy(), average='macro'))\n",
        "    return f1, acc, acc1\n",
        "\n",
        "model.train()\n",
        "for epoch in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        model.eval()\n",
        "        out = model(data.x, data.edge_index)\n",
        "        model.train()\n",
        "        f1_train, acc_train, acc1_train = calc_scores(out[data.train_mask], data.y[data.train_mask])\n",
        "        f1_test, acc_test, acc1_test = calc_scores(out[data.test_mask], data.y[data.test_mask])\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}, '\n",
        "              f'f1/train: {f1_train:.2f}, acc/avg/train: {acc_train:.2f}, acc/1/train: {acc1_train:.2f}, '\n",
        "              f'f1/test: {f1_test:.2f}, acc/avg/test: {acc_test:.2f}, acc/1/test: {acc1_test:.2f}')\n",
        "\n",
        "def save_to_qgis(data_, pred_):\n",
        "    data_out = []\n",
        "    for i, (node_id, node_data) in enumerate(G.nodes(data=True)):\n",
        "        data_out.append([\n",
        "            node_id,\n",
        "            node_data['label'],\n",
        "            node_data.get('square', 1),\n",
        "            node_data['kindergarten'],\n",
        "            node_data['school'],\n",
        "            node_data['bus_buf'],\n",
        "            node_data['tram_stop'],\n",
        "            node_data['subway_buf'],\n",
        "            node_data['label'],\n",
        "            node_data['education'],\n",
        "            node_data['coordinate'][1],\n",
        "            node_data['coordinate'][0],\n",
        "            pred_[i]\n",
        "        ])\n",
        "    df_out = pd.DataFrame(data_out, columns=['building_id', 'label', 'square', 'kindergarten', 'school', 'bus_buf', 'subway_buf', 'tram_stop',\n",
        "                                              'label', 'education', 'lat', 'lon', 'predict'])\n",
        "    print('saving...')\n",
        "    df_out.iloc[data_.train_mask].to_csv('/content/train2.csv', index=False)\n",
        "    df_out.iloc[data_.test_mask].to_csv('/content/test2.csv', index=False)\n",
        "\n",
        "model_filename = 'model.pth'\n",
        "torch.save(model.state_dict(), model_filename)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred = model(data.x, data.edge_index)\n",
        "    f1, acc_test, acc1_test = calc_scores(pred[data.test_mask], data.y[data.test_mask])\n",
        "    save_to_qgis(data, pred.max(1)[1].numpy())\n",
        "    print(f'TEST: f1: {f1:.2f}, acc/avg/test: {acc_test:.2f}, acc/1/test: {acc1_test:.2f}')"
      ],
      "metadata": {
        "id": "YWv4T-oSEbnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "application to new data"
      ],
      "metadata": {
        "id": "XYZL3bdHYYM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 step"
      ],
      "metadata": {
        "id": "GfU6MBacYeBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "buffer_size = 300\n",
        "n_closest = 3\n",
        "\n",
        "path_root_city = Path('/content')\n",
        "filename = 'data_high2.csv'\n",
        "filepath = path_root_city / filename\n",
        "filepath_subway = path_root_city / 'метро.geojson'\n",
        "filepath_bus = path_root_city / 'остановки автобусов.geojson'\n",
        "filepath_tram = path_root_city / 'остановки трамвай.geojson'\n",
        "\n",
        "df_squares = gpd.read_file(path_root_city / 'squares2.csv')\n",
        "df_squares['building_id'] = df_squares['building_id'].astype(int)\n",
        "\n",
        "def read_geojson(path: Path) -> Dict:\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def extract_stops(path: Path) -> List[Tuple[int, float, float]]:\n",
        "    data = read_geojson(path)\n",
        "    return [(int(f['properties']['osm_id']), f['geometry']['coordinates'][0], f['geometry']['coordinates'][1]) for f in data['features']]\n",
        "\n",
        "def draw_plotly(g_: nx.Graph) -> None:\n",
        "    pos_ = nx.get_node_attributes(g_, 'coordinate')\n",
        "    nodes = list(g_.nodes())\n",
        "\n",
        "    edge_x, edge_y, edges_text = [], [], []\n",
        "    for edge in g_.edges(data=True):\n",
        "        x0, y0 = pos_[edge[0]]\n",
        "        x1, y1 = pos_[edge[1]]\n",
        "        edge_x.extend([x0, x1, None])\n",
        "        edge_y.extend([y0, y1, None])\n",
        "        edges_text.extend([f\"Meters: {edge[2]['meters']}\", None])\n",
        "\n",
        "    node_x, node_y, node_text, node_colors = [], [], [], []\n",
        "    nodes_data = g_.nodes(data=True)\n",
        "    for node in nodes:\n",
        "        x, y = pos_[node]\n",
        "        node_x.append(x)\n",
        "        node_y.append(y)\n",
        "        attrs = nodes_data[node]\n",
        "        node_colors.append('black' if attrs['label'] in {'subway', 'bus', 'tram'} else 'green' if attrs['kindergarten'] > 0 else 'red')\n",
        "        filtered_attrs = {k: v for k, v in attrs.items() if k not in {'coordinate', 'label'}}\n",
        "        node_text.append('<br>'.join([f'{k}: {v}' for k, v in filtered_attrs.items()]))\n",
        "\n",
        "    df_nodes = pd.DataFrame.from_records([dict(nodes_data[node]) for node in nodes]).drop(columns='coordinate')\n",
        "    df_nodes['color'] = node_colors\n",
        "\n",
        "    fig = px.scatter_map(df_nodes, lat=\"lon\", lon=\"lat\", hover_name=\"kindergarten\", hover_data=[\"label\", \"square\"],\n",
        "                         color=df_nodes['color'], zoom=8, height=1000)\n",
        "    fig.update_layout(map_style=\"open-street-map\", margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0},\n",
        "                      title='Interactive Graph with NetworkX and Plotly', hovermode='closest')\n",
        "    fig.show()\n",
        "\n",
        "def calculate_distances(geometry: gpd.GeoSeries, point: Tuple[float, float]) -> pd.Series:\n",
        "    dst = gpd.GeoSeries([Point(point)], crs='epsg:4326').to_crs(epsg=3857)\n",
        "    return geometry.to_crs(epsg=3857).distance(dst[0])\n",
        "\n",
        "def add_stops(g_: nx.Graph, positions: Dict[int, Tuple[float, float]], stops: List[Tuple[int, float, float]], label: str) -> None:\n",
        "    geometry = [Point(xy) for xy in positions.values()]\n",
        "    gdf_pos = gpd.GeoDataFrame(list(zip(positions.keys(), positions.values())), columns=['id', 'geometry'], crs='epsg:4326', geometry=geometry)\n",
        "\n",
        "    for stop_id, lat, lon in stops:\n",
        "        distances = calculate_distances(gdf_pos.geometry, (lat, lon))\n",
        "        distances_less = distances[distances < buffer_size]\n",
        "        if distances_less.shape[0] > 0:\n",
        "            if g_.has_node(stop_id):\n",
        "                raise ValueError('The node already exists')\n",
        "            g_.add_node(stop_id, coordinate=(lat, lon), label=label, kindergarten=0, school=0, bus_buf=0, tram_stop=0, lat=lat, lon=lon)\n",
        "            for building_id in gdf_pos[distances < buffer_size]['id']:\n",
        "                g_.add_edge(building_id, stop_id, meters=0)\n",
        "\n",
        "def build_data() -> nx.Graph:\n",
        "    cache_file_path = Path('/content') / filename.replace('.csv', '.pkl')\n",
        "    if cache_file_path.exists():\n",
        "        with open(cache_file_path, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "    df = pd.read_csv(filepath)\n",
        "    data_json = {'features': df.to_dict(orient='records')}\n",
        "    stops_bus = extract_stops(filepath_bus)\n",
        "    stops_tram = extract_stops(filepath_tram)\n",
        "    stops_sub = extract_stops(filepath_subway)\n",
        "    graph = nx.Graph()\n",
        "    squares_map = df_squares.set_index('building_id')['building_area'].to_dict()\n",
        "\n",
        "    for i, building in enumerate(data_json['features']):\n",
        "        props = building['properties'] if 'properties' in building else building\n",
        "        building_id = props['building_id']\n",
        "        square = float(squares_map[building_id])\n",
        "        if square < 200:\n",
        "            continue\n",
        "        x_source, y_source = building['x'], building['y']\n",
        "        graph.add_node(building_id, coordinate=(x_source, y_source), square=square,\n",
        "                       label='Y' if props['kindergarten'] else 'N', kindergarten=props['kindergarten'],\n",
        "                       school=props['school'], bus_buf=props['bus_buf'], tram_stop=props['tram_stop'],\n",
        "                       subway_buf=props['subway_buf'], lat=x_source, lon=y_source)\n",
        "\n",
        "    pos_ = nx.get_node_attributes(graph, 'coordinate')\n",
        "    geometry = [Point(xy) for xy in pos_.values()]\n",
        "    gdf_pos = gpd.GeoDataFrame(list(zip(pos_.keys(), pos_.values())), columns=['id', 'geometry'], crs='epsg:4326', geometry=geometry)\n",
        "\n",
        "    for i, (building_id, coord) in enumerate(pos_.items()):\n",
        "        gdf_pos['distances'] = calculate_distances(gdf_pos.geometry, coord)\n",
        "        indices = (gdf_pos['distances'] > 0) & (gdf_pos['distances'] < buffer_size)\n",
        "        gdf_close = gdf_pos[indices] if indices.any() else gdf_pos.sort_values('distances')[1:n_closest]\n",
        "\n",
        "        for next_building_id, meters in zip(gdf_close['id'], gdf_close['distances']):\n",
        "            if next_building_id != building_id:\n",
        "                graph.add_edge(building_id, next_building_id, meters=meters)\n",
        "\n",
        "    add_stops(graph, pos_, stops_bus, 'bus')\n",
        "    add_stops(graph, pos_, stops_tram, 'tram')\n",
        "    add_stops(graph, pos_, stops_sub, 'subway')\n",
        "\n",
        "    with open(cache_file_path, 'wb') as f:\n",
        "        pickle.dump(graph, f)\n",
        "    return graph\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    graph_data = build_data()\n",
        "    draw_plotly(graph_data)"
      ],
      "metadata": {
        "id": "hYj_DG-PYcTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 step"
      ],
      "metadata": {
        "id": "H5N5rcRVYtPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, n_features):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(n_features, 32)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(32)\n",
        "        self.conv2 = GCNConv(32, 64)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(64)\n",
        "        self.conv3 = GCNConv(64, 128)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(128)\n",
        "        self.conv4 = GCNConv(128, 256)\n",
        "        self.bn4 = torch.nn.BatchNorm1d(256)\n",
        "        self.conv5 = GCNConv(256, 512)\n",
        "        self.bn5 = torch.nn.BatchNorm1d(512)\n",
        "        self.conv6 = GCNConv(512, 1024)\n",
        "        self.bn6 = torch.nn.BatchNorm1d(1024)\n",
        "        self.conv7 = GCNConv(1024, 2)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv4(x, edge_index)\n",
        "        x = self.bn4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv5(x, edge_index)\n",
        "        x = self.bn5(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv6(x, edge_index)\n",
        "        x = self.bn6(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv7(x, edge_index)\n",
        "        return x\n",
        "\n",
        "def read_data(filename):\n",
        "    with open(Path('/content') / filename, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def prepare_data(G):\n",
        "    features = []\n",
        "    node_mapping = {}\n",
        "    for i, (node_id, node_data) in enumerate(G.nodes(data=True)):\n",
        "        node_mapping[node_id] = i\n",
        "        features.append([\n",
        "            node_data.get('square', 1),\n",
        "            node_data.get('building_area', 1),\n",
        "            node_data.get('living_area', 1),\n",
        "            node_data.get('population_balanced', 1),\n",
        "            node_data['school'],\n",
        "            node_data['kindergarten'],\n",
        "            node_data['bus_buf'],\n",
        "            node_data['tram_stop'],\n",
        "            node_data.get('subway_buf', 0)\n",
        "        ])\n",
        "    features = torch.tensor(features, dtype=torch.float)\n",
        "    edges = [(node_mapping[v[0]], node_mapping[v[1]]) for v in G.edges]\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "    return Data(x=features, edge_index=edge_index), node_mapping\n",
        "\n",
        "def predict_and_save(model_path, graph_path, output_path):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    G = read_data(graph_path)\n",
        "    data, node_mapping = prepare_data(G)\n",
        "\n",
        "    model = GCN(data.num_features).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred = model(data.x.to(device), data.edge_index.to(device))\n",
        "        pred_labels = pred.max(1)[1].cpu().numpy()\n",
        "\n",
        "    results = []\n",
        "    for node_id, node_data in G.nodes(data=True):\n",
        "        results.append([\n",
        "            node_id,\n",
        "            node_data['label'],\n",
        "            node_data.get('square', 1),\n",
        "            node_data['kindergarten'],\n",
        "            node_data['school'],\n",
        "            node_data['bus_buf'],\n",
        "            node_data.get('subway_buf', 0),\n",
        "            node_data['tram_stop'],\n",
        "            node_data.get('education', 0),\n",
        "            node_data['coordinate'][1],\n",
        "            node_data['coordinate'][0],\n",
        "            pred_labels[node_mapping[node_id]]\n",
        "        ])\n",
        "\n",
        "    df = pd.DataFrame(results, columns=[\n",
        "        'building_id', 'label', 'square', 'kindergarten', 'school',\n",
        "        'bus_buf', 'subway_buf', 'tram_stop', 'education', 'y', 'x', 'predict'\n",
        "    ])\n",
        "    df.to_csv(output_path, index=False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    predict_and_save(\n",
        "        model_path='model.pth',\n",
        "        graph_path='data_high2.pkl',\n",
        "        output_path='/content/predictions_education.csv'\n",
        "    )"
      ],
      "metadata": {
        "id": "hBzmtfJwYviv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 2 - Food servecies predict"
      ],
      "metadata": {
        "id": "KMGS_CGPGsfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "code to create graph"
      ],
      "metadata": {
        "id": "3ETTQKMbGykX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "buffer_size = 300\n",
        "n_closest = 3\n",
        "path_root_city = Path('/content')\n",
        "filename = 'data_high3.csv'\n",
        "filepath = path_root_city / filename\n",
        "filepath_subway = path_root_city / 'метро.geojson'\n",
        "filepath_bus = path_root_city / 'остановки автобусов.geojson'\n",
        "filepath_tram = path_root_city / 'остановки трамвай.geojson'\n",
        "df_squares = gpd.read_file(path_root_city / 'squares1.csv')\n",
        "df_squares['building_id'] = df_squares['building_id'].astype(int)\n",
        "\n",
        "def read_geojson(path: Path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def extract_stops(path: Path) -> list[tuple[int, float, float]]:\n",
        "    data = read_geojson(path)\n",
        "    coords = list(map(lambda v: (int(v['properties']['osm_id']),\n",
        "                               v['geometry']['coordinates'][0],\n",
        "                               v['geometry']['coordinates'][1]), data['features']))\n",
        "    return coords\n",
        "\n",
        "def draw_plotly(g_):\n",
        "    pos_ = nx.get_node_attributes(g_, 'coordinate')\n",
        "    nodes = list(g_.nodes())\n",
        "    edge_x = []\n",
        "    edge_y = []\n",
        "    edges_text = []\n",
        "    for edge in g_.edges(data=True):\n",
        "        x0, y0 = pos_[edge[0]]\n",
        "        x1, y1 = pos_[edge[1]]\n",
        "        edge_x.extend([x0, x1, None])\n",
        "        edge_y.extend([y0, y1, None])\n",
        "        edges_text.extend([f\"Meters: {edge[2]['meters']}', f'Meters: {edge[2]['meters']}\", None])\n",
        "    nodes_data = g_.nodes(data=True)\n",
        "    node_x = []\n",
        "    node_y = []\n",
        "    node_text = []\n",
        "    node_colors = []\n",
        "    for node in nodes:\n",
        "        x, y = pos_[node]\n",
        "        node_x.append(x)\n",
        "        node_y.append(y)\n",
        "        attrs = nodes_data[node]\n",
        "        if attrs['label'] in {'subway', 'bus', 'tram'}:\n",
        "            node_colors.append('black')\n",
        "        else:\n",
        "            node_colors.append('green' if attrs['food'] > 0 else 'red')\n",
        "        attrs = dict(filter(lambda kv: kv[0] not in {'coordinate', 'label'}, attrs.items()))\n",
        "        node_text.append('<br>'.join(list(map(lambda kv: f'{kv[0]}: {kv[1]}', attrs.items()))))\n",
        "    df_nodes = pd.DataFrame.from_records(list(dict(nodes_data).values())).drop(columns='coordinate')\n",
        "    df_nodes['s_cnt'] = df_nodes[['kindergarten', 'bus_buf', 'school', 'tram_stop', 'subway_buf', 'education', 'edu_buf']].sum(axis=1)\n",
        "    df_nodes['color'] = 'gray'\n",
        "    df_nodes.loc[df_nodes['s_cnt'] > 0, 'color'] = 'blue'\n",
        "    df_nodes.loc[df_nodes['food'] == 1, 'color'] = 'green'\n",
        "    df_nodes.loc[df_nodes['label'].isin({'tram', 'subway', 'bus'}), 'color'] = 'black'\n",
        "    fig = px.scatter_map(df_nodes, lat=\"lon\", lon=\"lat\", hover_name=\"food\", hover_data=[\"label\", \"square\"],\n",
        "                       color=df_nodes['color'],\n",
        "                       zoom=8,\n",
        "                       height=1000)\n",
        "    fig.update_layout(map_style=\"open-street-map\")\n",
        "    fig.update_layout(margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0})\n",
        "    fig.update_layout(\n",
        "        title='Interactive Graph with NetworkX and Plotly',\n",
        "        hovermode='closest',\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "def calculate_distances(geometry: gpd.GeoSeries, point: tuple[float, float]) -> pd.Series:\n",
        "    dst = gpd.GeoSeries([Point(point), ], crs='epsg:4326').to_crs(epsg=3857)\n",
        "    return geometry.to_crs(epsg=3857).distance(dst[0])\n",
        "\n",
        "def add_stops(\n",
        "        g_: nx.Graph,\n",
        "        positions: dict[int, tuple[float, float]],\n",
        "        stops: list[tuple[int, float, float]],\n",
        "        label: str\n",
        "):\n",
        "    geometry = [Point(xy) for xy in list(positions.values())]\n",
        "    gdf_pos = gpd.GeoDataFrame(list(zip(positions.keys(), positions.values())), columns=['id', 'geometry'], crs='epsg:4326', geometry=geometry)\n",
        "    for stop_id, lat, lon in stops:\n",
        "        distances = calculate_distances(gdf_pos.geometry, (lat, lon))\n",
        "        distances_less = distances[distances < buffer_size]\n",
        "        if distances_less.shape[0] > 0:\n",
        "            if g_.has_node(stop_id):\n",
        "                raise ValueError('The node already exists')\n",
        "            g_.add_node(stop_id, coordinate=(lat, lon),\n",
        "                      label=label, education=0, kindergarten=0, school=0, bus_buf=0, tram_stop=0, subway_buf=0, edu_buf=0, food=0,\n",
        "                      lat=lat, lon=lon)\n",
        "            for building_id in gdf_pos[distances < buffer_size]['id']:\n",
        "                g_.add_edge(building_id, stop_id, meters=0)\n",
        "\n",
        "def build_data():\n",
        "    cache_file_path = Path('/content') / filename.replace('.csv', '.pkl')\n",
        "    if cache_file_path.exists():\n",
        "        with open(cache_file_path, 'rb') as f:\n",
        "            graph = pickle.load(f)\n",
        "        return graph\n",
        "    df = pd.read_csv(filepath)\n",
        "    data_json = {'features': df.to_dict(orient='records')}\n",
        "    stops_bus = extract_stops(filepath_bus)\n",
        "    stops_tram = extract_stops(filepath_tram)\n",
        "    stops_sub = extract_stops(filepath_subway)\n",
        "    graph = nx.Graph()\n",
        "    squares_map = df_squares.set_index('building_id')['building_area'].to_dict()\n",
        "    n_count = len(data_json['features'])\n",
        "    for i, building in enumerate(data_json['features']):\n",
        "        if i % 100 == 0:\n",
        "            print(f'Calc {i} / {n_count}')\n",
        "        props = building['properties'] if 'properties' in building else building\n",
        "        building_id = props['building_id']\n",
        "        square = float(squares_map[building_id])\n",
        "        food = int(props['food'] > 0)\n",
        "        if square < 200:\n",
        "            print(f'Skip building \"{building_id}\" with square {square} and food={food}')\n",
        "            continue\n",
        "        x_source, y_source = building['x'], building['y']\n",
        "        graph.add_node(props['building_id'], coordinate=(x_source, y_source),\n",
        "                     square=square,\n",
        "                     label='Y' if props['food'] else 'N',\n",
        "                     kindergarten=props['kindergarten'],\n",
        "                     school=props['school'],\n",
        "                     bus_buf=props['bus_buf'],\n",
        "                     tram_stop=props['tram_stop'],\n",
        "                     subway_buf=props['subway_buf'],\n",
        "                     education=props['education'],\n",
        "                     edu_buf=props['edu_buf'],\n",
        "                     food=food,\n",
        "                     lat=x_source,\n",
        "                     lon=y_source)\n",
        "    print('The nodes was build')\n",
        "    pos_: dict[int, tuple[float, float]] = nx.get_node_attributes(graph, 'coordinate')\n",
        "    geometry = [Point(xy) for xy in list(pos_.values())]\n",
        "    gdf_pos = gpd.GeoDataFrame(list(zip(pos_.keys(), pos_.values())), columns=['id', 'geometry'], crs='epsg:4326', geometry=geometry)\n",
        "    n_count = len(pos_)\n",
        "    for i, (building_id, coord) in enumerate(pos_.items()):\n",
        "        if i % 100 == 0:\n",
        "            print(f'Calc {i} / {n_count}')\n",
        "        gdf_pos['distances'] = calculate_distances(gdf_pos.geometry, coord)\n",
        "        indices = (gdf_pos['distances'] > 0) & (gdf_pos['distances'] < buffer_size)\n",
        "        if indices.any():\n",
        "            gdf_close = gdf_pos[gdf_pos['distances'] < buffer_size]\n",
        "        else:\n",
        "            gdf_close = gdf_pos.sort_values('distances')[1:n_closest]\n",
        "        was_added_any = False\n",
        "        for next_building_id, meters in zip(gdf_close['id'], gdf_close['distances']):\n",
        "            if next_building_id == building_id:\n",
        "                continue\n",
        "            graph.add_edge(building_id, next_building_id, meters=meters)\n",
        "            was_added_any = True\n",
        "        if not was_added_any:\n",
        "            raise ValueError('...')\n",
        "    print('The graph was build')\n",
        "    add_stops(graph, pos_, stops_bus, 'bus')\n",
        "    print('The bus were added')\n",
        "    add_stops(graph, pos_, stops_tram, 'tram')\n",
        "    print('The tram were added')\n",
        "    add_stops(graph, pos_, stops_sub, 'subway')\n",
        "    print('The subway were added')\n",
        "    with open(cache_file_path, 'wb') as f:\n",
        "        pickle.dump(graph, f)\n",
        "    return graph\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    graph_data = build_data()\n",
        "    draw_plotly(graph_data)"
      ],
      "metadata": {
        "id": "zzH1z8F2GyLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "binary classification of graph using graph neural network GCN"
      ],
      "metadata": {
        "id": "D8OoO0AHG3gQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'data_high3.pkl'\n",
        "\n",
        "def read_data() -> nx.Graph:\n",
        "    cache_file_path = Path('/content') / filename\n",
        "    with open(cache_file_path, 'rb') as f:\n",
        "        graph = pickle.load(f)\n",
        "    return graph\n",
        "\n",
        "G: nx.Graph = read_data()\n",
        "N_FEATURES = 12\n",
        "\n",
        "features = []\n",
        "labels = []\n",
        "node_mapping = {}\n",
        "for i, (node_id, node_data) in enumerate(G.nodes(data=True)):\n",
        "    node_mapping[node_id] = i\n",
        "    is_stop = 0\n",
        "    if node_data['label'] == 'bus':\n",
        "        is_stop = 1\n",
        "    elif node_data['label'] == 'tram':\n",
        "        is_stop = 2\n",
        "    elif node_data['label'] == 'subway':\n",
        "        is_stop = 3\n",
        "    features.append([\n",
        "        node_data.get('square', 1),\n",
        "        node_data.get('building_area', 1),\n",
        "        node_data.get('living_area', 1),\n",
        "        node_data.get('population_balanced', 1),\n",
        "        node_data['school'],\n",
        "        node_data['kindergarten'],\n",
        "        node_data['bus_buf'],\n",
        "        node_data['tram_stop'],\n",
        "        node_data['subway_buf'],\n",
        "        node_data['edu_buf'],\n",
        "        node_data['education'],\n",
        "        is_stop\n",
        "    ])\n",
        "    labels.append(int(node_data['food'] > 0))\n",
        "\n",
        "features = torch.tensor(features, dtype=torch.float)\n",
        "num_nodes = features.size(0)\n",
        "labels = torch.tensor(labels, dtype=torch.long)\n",
        "train_indices, test_indices = train_test_split(\n",
        "    np.arange(num_nodes),\n",
        "    test_size=0.3,\n",
        "    stratify=labels.numpy(),\n",
        "    random_state=21\n",
        ")\n",
        "edges = list(map(lambda v: (node_mapping[v[0]], node_mapping[v[1]]), G.edges))\n",
        "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "data = Data(x=features, edge_index=edge_index, y=labels,\n",
        "            train_mask=train_indices, test_mask=test_indices)\n",
        "assert edge_index.max() < num_nodes, \"Edge index is out of bounds!\"\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(N_FEATURES, 32)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(32)\n",
        "        self.conv2 = GCNConv(32, 64)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(64)\n",
        "        self.conv3 = GCNConv(64, 128)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(128)\n",
        "        self.conv4 = GCNConv(128, 256)\n",
        "        self.bn4 = torch.nn.BatchNorm1d(256)\n",
        "        self.conv5 = GCNConv(256, 512)\n",
        "        self.bn5 = torch.nn.BatchNorm1d(512)\n",
        "        self.conv6 = GCNConv(512, 1024)\n",
        "        self.bn6 = torch.nn.BatchNorm1d(1024)\n",
        "        self.conv7 = GCNConv(1024, 2)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv4(x, edge_index)\n",
        "        x = self.bn4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv5(x, edge_index)\n",
        "        x = self.bn5(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv6(x, edge_index)\n",
        "        x = self.bn6(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv7(x, edge_index)\n",
        "        return x\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "model = GCN().to(device)\n",
        "data = data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "class_weights = torch.tensor([1., 10.])\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "def calc_scores(out_, target_) -> tuple[float, float, float]:\n",
        "    out_max = out_.max(1)[1]\n",
        "    correct = target_.eq(out_max).sum().item()\n",
        "    acc = correct / target_.size()[0]\n",
        "    correct1 = target_[target_.eq(1)].eq(out_max[target_.eq(1)]).sum().item()\n",
        "    acc1 = correct1 / target_[target_.eq(1)].size()[0]\n",
        "    f1 = float(f1_score(target_.numpy(), out_max.numpy(), average='macro'))\n",
        "    return f1, acc, acc1\n",
        "\n",
        "model.train()\n",
        "for epoch in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        model.eval()\n",
        "        out = model(data.x, data.edge_index)\n",
        "        model.train()\n",
        "        f1_train, acc_train, acc1_train = calc_scores(out[data.train_mask], data.y[data.train_mask])\n",
        "        f1_test, acc_test, acc1_test = calc_scores(out[data.test_mask], data.y[data.test_mask])\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}, '\n",
        "              f'f1/train: {f1_train:.2f}, acc/avg/train: {acc_train:.2f}, acc/1/train: {acc1_train:.2f}, '\n",
        "              f'f1/test: {f1_test:.2f}, acc/avg/test: {acc_test:.2f}, acc/1/test: {acc1_test:.2f}')\n",
        "\n",
        "def save_to_qgis(data_, pred_):\n",
        "    data_out = []\n",
        "    for i, (node_id, node_data) in enumerate(G.nodes(data=True)):\n",
        "        data_out.append([\n",
        "            node_id,\n",
        "            node_data['label'],\n",
        "            node_data.get('square', 1),\n",
        "            node_data['kindergarten'],\n",
        "            node_data['school'],\n",
        "            node_data['bus_buf'],\n",
        "            node_data['tram_stop'],\n",
        "            node_data['subway_buf'],\n",
        "            node_data['label'],\n",
        "            node_data['education'],\n",
        "            node_data['edu_buf'],\n",
        "            node_data['food'],\n",
        "            node_data['coordinate'][1],\n",
        "            node_data['coordinate'][0],\n",
        "            pred_[i]\n",
        "        ])\n",
        "    df_out = pd.DataFrame(data_out, columns=['building_id', 'label', 'square', 'kindergarten', 'school', 'bus_buf', 'subway_buf', 'tram_stop',\n",
        "                                            'label', 'education', 'edu_buf', 'food', 'lat', 'lon', 'predict'])\n",
        "    print('saving...')\n",
        "    df_out.iloc[data_.train_mask].to_csv('/content/train3.csv', index=False)\n",
        "    df_out.iloc[data_.test_mask].to_csv('/content/test3.csv', index=False)\n",
        "\n",
        "model_filename = 'model2.pth'\n",
        "torch.save(model.state_dict(), model_filename)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred = model(data.x, data.edge_index)\n",
        "    f1, acc_test, acc1_test = calc_scores(pred[data.test_mask], data.y[data.test_mask])\n",
        "    save_to_qgis(data, pred.max(1)[1].numpy())\n",
        "    print(f'TEST: f1: {f1:.2f}, acc/avg/test: {acc_test:.2f}, acc/1/test: {acc1_test:.2f}')"
      ],
      "metadata": {
        "id": "ZEwV2xzKG3Jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "application to new data"
      ],
      "metadata": {
        "id": "Dx7jSEL6Yyqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 step"
      ],
      "metadata": {
        "id": "5JPzWrwaY2ms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from typing import Tuple, List, Dict\n",
        "import geopandas as gpd\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from shapely.geometry import Point\n",
        "\n",
        "buffer_size = 300\n",
        "n_closest = 3\n",
        "path_root_city = Path('/content')\n",
        "filename = 'data_high4.csv'\n",
        "filepath = path_root_city / filename\n",
        "filepath_subway = path_root_city / 'метро.geojson'\n",
        "filepath_bus = path_root_city / 'остановки автобусов.geojson'\n",
        "filepath_tram = path_root_city / 'остановки трамвай.geojson'\n",
        "df_squares = gpd.read_file(path_root_city / 'squares2.csv')\n",
        "df_squares['building_id'] = df_squares['building_id'].astype(int)\n",
        "\n",
        "def read_geojson(path: Path) -> Dict:\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        return json.load(f)\n",
        "\n",
        "def extract_stops(path: Path) -> List[Tuple[int, float, float]]:\n",
        "    data = read_geojson(path)\n",
        "    return [(int(f['properties']['osm_id']), f['geometry']['coordinates'][0], f['geometry']['coordinates'][1]) for f in data['features']]\n",
        "\n",
        "def draw_plotly(g_: nx.Graph) -> None:\n",
        "    pos_ = nx.get_node_attributes(g_, 'coordinate')\n",
        "    nodes = list(g_.nodes())\n",
        "\n",
        "    edge_x, edge_y, edges_text = [], [], []\n",
        "    for edge in g_.edges(data=True):\n",
        "        x0, y0 = pos_[edge[0]]\n",
        "        x1, y1 = pos_[edge[1]]\n",
        "        edge_x.extend([x0, x1, None])\n",
        "        edge_y.extend([y0, y1, None])\n",
        "        edges_text.extend([f\"Meters: {edge[2]['meters']}\", None])\n",
        "\n",
        "    node_x, node_y, node_text, node_colors = [], [], [], []\n",
        "    nodes_data = g_.nodes(data=True)\n",
        "    for node in nodes:\n",
        "        x, y = pos_[node]\n",
        "        node_x.append(x)\n",
        "        node_y.append(y)\n",
        "        attrs = nodes_data[node]\n",
        "        node_colors.append('black' if attrs['label'] in {'subway', 'bus', 'tram'} else 'green' if attrs['kindergarten'] > 0 else 'red')\n",
        "        filtered_attrs = {k: v for k, v in attrs.items() if k not in {'coordinate', 'label'}}\n",
        "        node_text.append('<br>'.join([f'{k}: {v}' for k, v in filtered_attrs.items()]))\n",
        "\n",
        "    df_nodes = pd.DataFrame.from_records([dict(nodes_data[node]) for node in nodes]).drop(columns='coordinate')\n",
        "    df_nodes['color'] = node_colors\n",
        "\n",
        "    fig = px.scatter_map(df_nodes, lat=\"lon\", lon=\"lat\", hover_name=\"kindergarten\", hover_data=[\"label\", \"square\"],\n",
        "                         color=df_nodes['color'], zoom=8, height=1000)\n",
        "    fig.update_layout(map_style=\"open-street-map\", margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0},\n",
        "                      title='Interactive Graph with NetworkX and Plotly', hovermode='closest')\n",
        "    fig.show()\n",
        "\n",
        "def calculate_distances(geometry: gpd.GeoSeries, point: Tuple[float, float]) -> pd.Series:\n",
        "    dst = gpd.GeoSeries([Point(point)], crs='epsg:4326').to_crs(epsg=3857)\n",
        "    return geometry.to_crs(epsg=3857).distance(dst[0])\n",
        "\n",
        "def add_stops(g_: nx.Graph, positions: Dict[int, Tuple[float, float]], stops: List[Tuple[int, float, float]], label: str) -> None:\n",
        "    geometry = [Point(xy) for xy in positions.values()]\n",
        "    gdf_pos = gpd.GeoDataFrame(list(zip(positions.keys(), positions.values())), columns=['id', 'geometry'], crs='epsg:4326', geometry=geometry)\n",
        "\n",
        "    for stop_id, lat, lon in stops:\n",
        "        distances = calculate_distances(gdf_pos.geometry, (lat, lon))\n",
        "        distances_less = distances[distances < buffer_size]\n",
        "        if distances_less.shape[0] > 0:\n",
        "            if g_.has_node(stop_id):\n",
        "                raise ValueError('Node already exists')\n",
        "            g_.add_node(stop_id, coordinate=(lat, lon), label=label, kindergarten=0, school=0,\n",
        "                       bus_buf=0, tram_stop=0, subway_buf=0, lat=lat, lon=lon, education=0, edu_buf=0)\n",
        "            for building_id in gdf_pos[distances < buffer_size]['id']:\n",
        "                g_.add_edge(building_id, stop_id, meters=0)\n",
        "\n",
        "def build_data() -> nx.Graph:\n",
        "    cache_file_path = Path('/content') / filename.replace('.csv', '.pkl')\n",
        "    if cache_file_path.exists():\n",
        "        with open(cache_file_path, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "    df = pd.read_csv(filepath)\n",
        "    data_json = {'features': df.to_dict(orient='records')}\n",
        "    stops_bus = extract_stops(filepath_bus)\n",
        "    stops_tram = extract_stops(filepath_tram)\n",
        "    stops_sub = extract_stops(filepath_subway)\n",
        "    graph = nx.Graph()\n",
        "    squares_map = df_squares.set_index('building_id')['building_area'].to_dict()\n",
        "\n",
        "    for i, building in enumerate(data_json['features']):\n",
        "        props = building.get('properties', building)\n",
        "        building_id = props['building_id']\n",
        "        square = float(squares_map[building_id])\n",
        "\n",
        "        if square < 200:\n",
        "            continue\n",
        "\n",
        "        x_source, y_source = building['x'], building['y']\n",
        "        graph.add_node(building_id, coordinate=(x_source, y_source), square=square,\n",
        "                      label='Y' if props['kindergarten'] else 'N', kindergarten=props['kindergarten'],\n",
        "                      school=props['school'], bus_buf=props['bus_buf'], tram_stop=props['tram_stop'],\n",
        "                      subway_buf=props['subway_buf'], education=props['education'], edu_buf=props['edu_buf'],\n",
        "                      lat=x_source, lon=y_source)\n",
        "\n",
        "    pos_ = nx.get_node_attributes(graph, 'coordinate')\n",
        "    geometry = [Point(xy) for xy in pos_.values()]\n",
        "    gdf_pos = gpd.GeoDataFrame(list(zip(pos_.keys(), pos_.values())), columns=['id', 'geometry'], crs='epsg:4326', geometry=geometry)\n",
        "\n",
        "    for i, (building_id, coord) in enumerate(pos_.items()):\n",
        "        gdf_pos['distances'] = calculate_distances(gdf_pos.geometry, coord)\n",
        "        indices = (gdf_pos['distances'] > 0) & (gdf_pos['distances'] < buffer_size)\n",
        "        gdf_close = gdf_pos[indices] if indices.any() else gdf_pos.sort_values('distances')[1:n_closest]\n",
        "\n",
        "        for next_building_id, meters in zip(gdf_close['id'], gdf_close['distances']):\n",
        "            if next_building_id != building_id:\n",
        "                graph.add_edge(building_id, next_building_id, meters=meters)\n",
        "\n",
        "    add_stops(graph, pos_, stops_bus, 'bus')\n",
        "    add_stops(graph, pos_, stops_tram, 'tram')\n",
        "    add_stops(graph, pos_, stops_sub, 'subway')\n",
        "\n",
        "    with open(cache_file_path, 'wb') as f:\n",
        "        pickle.dump(graph, f)\n",
        "    return graph\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    graph_data = build_data()\n",
        "    draw_plotly(graph_data)"
      ],
      "metadata": {
        "id": "ju8XFL7jY3mK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2 step"
      ],
      "metadata": {
        "id": "j2VtOEgQY39a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, n_features: int):\n",
        "        super(GCN, self).__init__()\n",
        "        self.layers = torch.nn.ModuleList([\n",
        "            GCNConv(n_features, 32),\n",
        "            GCNConv(32, 64),\n",
        "            GCNConv(64, 128),\n",
        "            GCNConv(128, 256),\n",
        "            GCNConv(256, 512),\n",
        "            GCNConv(512, 1024),\n",
        "            GCNConv(1024, 2)\n",
        "        ])\n",
        "        self.batch_norms = torch.nn.ModuleList([\n",
        "            torch.nn.BatchNorm1d(32),\n",
        "            torch.nn.BatchNorm1d(64),\n",
        "            torch.nn.BatchNorm1d(128),\n",
        "            torch.nn.BatchNorm1d(256),\n",
        "            torch.nn.BatchNorm1d(512),\n",
        "            torch.nn.BatchNorm1d(1024)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x: torch.Tensor, edge_index: torch.Tensor) -> torch.Tensor:\n",
        "        for i, (conv, bn) in enumerate(zip(self.layers[:-1], self.batch_norms)):\n",
        "            x = conv(x, edge_index)\n",
        "            x = bn(x)\n",
        "            x = F.relu(x)\n",
        "        return self.layers[-1](x, edge_index)\n",
        "\n",
        "def read_data(filename: str) -> nx.Graph:\n",
        "    with open(Path('/content') / filename, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def prepare_graph_data(G: nx.Graph) -> tuple[Data, dict]:\n",
        "    features, node_mapping = [], {}\n",
        "    for i, (node_id, node_data) in enumerate(G.nodes(data=True)):\n",
        "        node_mapping[node_id] = i\n",
        "        stop_type = {\n",
        "            'bus': 1,\n",
        "            'tram': 2,\n",
        "            'subway': 3\n",
        "        }.get(node_data['label'], 0)\n",
        "\n",
        "        features.append([\n",
        "            node_data.get('square', 1),\n",
        "            node_data.get('building_area', 1),\n",
        "            node_data.get('living_area', 1),\n",
        "            node_data.get('population_balanced', 1),\n",
        "            node_data['school'],\n",
        "            node_data['kindergarten'],\n",
        "            node_data['bus_buf'],\n",
        "            node_data['tram_stop'],\n",
        "            node_data['subway_buf'],\n",
        "            node_data['education'],\n",
        "            node_data['edu_buf'],\n",
        "            stop_type\n",
        "        ])\n",
        "\n",
        "    features = torch.tensor(features, dtype=torch.float)\n",
        "    edges = [(node_mapping[u], node_mapping[v]) for u, v in G.edges()]\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "    return Data(x=features, edge_index=edge_index), node_mapping\n",
        "\n",
        "def predict_and_save(model_path: str, graph_path: str, output_path: str) -> None:\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    G = read_data(graph_path)\n",
        "    data, node_mapping = prepare_graph_data(G)\n",
        "\n",
        "    model = GCN(data.num_features).to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred = model(data.x.to(device), data.edge_index.to(device))\n",
        "        pred_labels = pred.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "    results = []\n",
        "    for node_id, node_data in G.nodes(data=True):\n",
        "        results.append([\n",
        "            node_id,\n",
        "            node_data['label'],\n",
        "            node_data.get('square', 1),\n",
        "            node_data['kindergarten'],\n",
        "            node_data['school'],\n",
        "            node_data['bus_buf'],\n",
        "            node_data['subway_buf'],\n",
        "            node_data['tram_stop'],\n",
        "            node_data['education'],\n",
        "            node_data['edu_buf'],\n",
        "            node_data.get('food', 0),\n",
        "            node_data['coordinate'][1],\n",
        "            node_data['coordinate'][0],\n",
        "            pred_labels[node_mapping[node_id]]\n",
        "        ])\n",
        "\n",
        "    pd.DataFrame(results, columns=[\n",
        "        'building_id', 'label', 'square', 'kindergarten', 'school',\n",
        "        'bus_buf', 'tram_stop', 'subway_buf', 'education',\n",
        "        'edu_buf', 'food', 'y', 'x', 'predict'\n",
        "    ]).to_csv(output_path, index=False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    predict_and_save(\n",
        "        model_path='model2.pth',\n",
        "        graph_path='data_high4.pkl',\n",
        "        output_path='/content/predictions_food.csv'\n",
        "    )"
      ],
      "metadata": {
        "id": "3JfbgvgxY5Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stage 3 - Commerce servecies predict"
      ],
      "metadata": {
        "id": "t11CNdowG6va"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "code to create graph"
      ],
      "metadata": {
        "id": "JzHqvCyBG-e_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "buffer_size = 300\n",
        "n_closest = 3\n",
        "path_root_city = Path('/content')\n",
        "filename = 'data_high5.csv'\n",
        "filepath = path_root_city / filename\n",
        "filepath_subway = path_root_city / 'метро.geojson'\n",
        "filepath_bus = path_root_city / 'остановки автобусов.geojson'\n",
        "filepath_tram = path_root_city / 'остановки трамвай.geojson'\n",
        "df_squares = gpd.read_file(path_root_city / 'squares1.csv')\n",
        "df_squares['building_id'] = df_squares['building_id'].astype(int)\n",
        "\n",
        "def read_geojson(path: Path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    return data\n",
        "\n",
        "def extract_stops(path: Path) -> list[tuple[int, float, float]]:\n",
        "    data = read_geojson(path)\n",
        "    coords = list(map(lambda v: (int(v['properties']['osm_id']),\n",
        "                     v['geometry']['coordinates'][0],\n",
        "                     v['geometry']['coordinates'][1]), data['features']))\n",
        "    return coords\n",
        "\n",
        "def draw_plotly(g_):\n",
        "    pos_ = nx.get_node_attributes(g_, 'coordinate')\n",
        "    nodes = list(g_.nodes())\n",
        "    edge_x = []\n",
        "    edge_y = []\n",
        "    edges_text = []\n",
        "    for edge in g_.edges(data=True):\n",
        "        x0, y0 = pos_[edge[0]]\n",
        "        x1, y1 = pos_[edge[1]]\n",
        "        edge_x.extend([x0, x1, None])\n",
        "        edge_y.extend([y0, y1, None])\n",
        "        edges_text.extend([f\"Meters: {edge[2]['meters']}\", f\"Meters: {edge[2]['meters']}\", None])\n",
        "\n",
        "    nodes_data = g_.nodes(data=True)\n",
        "    df_nodes = pd.DataFrame.from_records(list(dict(nodes_data).values())).drop(columns='coordinate')\n",
        "    df_nodes['s_cnt'] = df_nodes[['kindergarten', 'bus_buf', 'school', 'tram_stop', 'subway_buf', 'education', 'edu_buf']].sum(axis=1)\n",
        "    df_nodes['color'] = 'gray'\n",
        "    df_nodes.loc[df_nodes['s_cnt'] > 0, 'color'] = 'blue'\n",
        "    df_nodes.loc[df_nodes['commerce'] == 1, 'color'] = 'green'\n",
        "    df_nodes.loc[df_nodes['label'].isin({'tram', 'subway', 'bus'}), 'color'] = 'black'\n",
        "\n",
        "    fig = px.scatter_map(df_nodes, lat=\"lon\", lon=\"lat\", hover_name=\"commerce\",\n",
        "                        hover_data=[\"label\", \"square\"], color=df_nodes['color'],\n",
        "                        zoom=8, height=1000)\n",
        "    fig.update_layout(map_style=\"open-street-map\")\n",
        "    fig.update_layout(margin={\"r\":0, \"t\":0, \"l\":0, \"b\":0})\n",
        "    fig.update_layout(title='Interactive Graph with NetworkX and Plotly', hovermode='closest')\n",
        "    fig.show()\n",
        "\n",
        "def calculate_distances(geometry: gpd.GeoSeries, point: tuple[float, float]) -> pd.Series:\n",
        "    dst = gpd.GeoSeries([Point(point)], crs='epsg:4326').to_crs(epsg=3857)\n",
        "    return geometry.to_crs(epsg=3857).distance(dst[0])\n",
        "\n",
        "def add_stops(g_: nx.Graph, positions: dict[int, tuple[float, float]],\n",
        "             stops: list[tuple[int, float, float]], label: str):\n",
        "    geometry = [Point(xy) for xy in list(positions.values())]\n",
        "    gdf_pos = gpd.GeoDataFrame(list(zip(positions.keys(), positions.values())),\n",
        "                              columns=['id', 'geometry'], crs='epsg:4326', geometry=geometry)\n",
        "\n",
        "    for stop_id, lat, lon in stops:\n",
        "        distances = calculate_distances(gdf_pos.geometry, (lat, lon))\n",
        "        distances_less = distances[distances < buffer_size]\n",
        "        if distances_less.shape[0] > 0:\n",
        "            if g_.has_node(stop_id):\n",
        "                raise ValueError('The node already exists')\n",
        "            g_.add_node(stop_id, coordinate=(lat, lon),\n",
        "                       label=label, education=0, kindergarten=0, school=0,\n",
        "                       bus_buf=0, tram_stop=0, subway_buf=0, edu_buf=0,\n",
        "                       food=0, commerce=0, food_buf=0, lat=lat, lon=lon)\n",
        "            for building_id in gdf_pos[distances < buffer_size]['id']:\n",
        "                g_.add_edge(building_id, stop_id, meters=0)\n",
        "\n",
        "def build_data():\n",
        "    cache_file_path = Path('/content') / filename.replace('.csv', '.pkl')\n",
        "    if cache_file_path.exists():\n",
        "        with open(cache_file_path, 'rb') as f:\n",
        "            graph = pickle.load(f)\n",
        "        return graph\n",
        "\n",
        "    df = pd.read_csv(filepath)\n",
        "    data_json = {'features': df.to_dict(orient='records')}\n",
        "    stops_bus = extract_stops(filepath_bus)\n",
        "    stops_tram = extract_stops(filepath_tram)\n",
        "    stops_sub = extract_stops(filepath_subway)\n",
        "    graph = nx.Graph()\n",
        "    squares_map = df_squares.set_index('building_id')['building_area'].to_dict()\n",
        "\n",
        "    n_count = len(data_json['features'])\n",
        "    for i, building in enumerate(data_json['features']):\n",
        "        if i % 100 == 0:\n",
        "            print(f'Calc {i} / {n_count}')\n",
        "\n",
        "        props = building['properties'] if 'properties' in building else building\n",
        "        building_id = props['building_id']\n",
        "        square = float(squares_map[building_id])\n",
        "        commerce = int(props['commerce'] > 0)\n",
        "\n",
        "        if square < 200:\n",
        "            print(f'Skip building \"{building_id}\" with square {square} and commerce={commerce}')\n",
        "            continue\n",
        "\n",
        "        x_source, y_source = building['x'], building['y']\n",
        "        graph.add_node(props['building_id'], coordinate=(x_source, y_source),\n",
        "                      square=square, label='Y' if props['commerce'] else 'N',\n",
        "                      kindergarten=props['kindergarten'], school=props['school'],\n",
        "                      bus_buf=props['bus_buf'], tram_stop=props['tram_stop'],\n",
        "                      subway_buf=props['subway_buf'], education=props['education'],\n",
        "                      edu_buf=props['edu_buf'], food_buf=props['food_buf'],\n",
        "                      food=props['food'], commerce=commerce, lat=x_source, lon=y_source)\n",
        "\n",
        "    print('The nodes was build')\n",
        "    pos_ = nx.get_node_attributes(graph, 'coordinate')\n",
        "    geometry = [Point(xy) for xy in list(pos_.values())]\n",
        "    gdf_pos = gpd.GeoDataFrame(list(zip(pos_.keys(), pos_.values())),\n",
        "                              columns=['id', 'geometry'], crs='epsg:4326', geometry=geometry)\n",
        "\n",
        "    n_count = len(pos_)\n",
        "    for i, (building_id, coord) in enumerate(pos_.items()):\n",
        "        if i % 100 == 0:\n",
        "            print(f'Calc {i} / {n_count}')\n",
        "\n",
        "        gdf_pos['distances'] = calculate_distances(gdf_pos.geometry, coord)\n",
        "        indices = (gdf_pos['distances'] > 0) & (gdf_pos['distances'] < buffer_size)\n",
        "        gdf_close = gdf_pos[gdf_pos['distances'] < buffer_size] if indices.any() else gdf_pos.sort_values('distances')[1:n_closest]\n",
        "\n",
        "        was_added_any = False\n",
        "        for next_building_id, meters in zip(gdf_close['id'], gdf_close['distances']):\n",
        "            if next_building_id != building_id:\n",
        "                graph.add_edge(building_id, next_building_id, meters=meters)\n",
        "                was_added_any = True\n",
        "\n",
        "        if not was_added_any:\n",
        "            raise ValueError('No edges added')\n",
        "\n",
        "    print('The graph was build')\n",
        "    add_stops(graph, pos_, stops_bus, 'bus')\n",
        "    print('The bus were added')\n",
        "    add_stops(graph, pos_, stops_tram, 'tram')\n",
        "    print('The tram were added')\n",
        "    add_stops(graph, pos_, stops_sub, 'subway')\n",
        "    print('The subway were added')\n",
        "\n",
        "    with open(cache_file_path, 'wb') as f:\n",
        "        pickle.dump(graph, f)\n",
        "    return graph\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    graph_data = build_data()\n",
        "    draw_plotly(graph_data)"
      ],
      "metadata": {
        "id": "KTqJpU0UIzDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "binary classification of graph using graph neural network GCN"
      ],
      "metadata": {
        "id": "UhzRajgfG_4r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'data_high5.pkl'\n",
        "\n",
        "def read_data() -> nx.Graph:\n",
        "    cache_file_path = Path('/content') / filename\n",
        "    with open(cache_file_path, 'rb') as f:\n",
        "        graph = pickle.load(f)\n",
        "    return graph\n",
        "\n",
        "G: nx.Graph = read_data()\n",
        "N_FEATURES = 14\n",
        "\n",
        "features = []\n",
        "labels = []\n",
        "node_mapping = {}\n",
        "for i, (node_id, node_data) in enumerate(G.nodes(data=True)):\n",
        "    node_mapping[node_id] = i\n",
        "    is_stop = 0\n",
        "    if node_data['label'] == 'bus':\n",
        "        is_stop = 1\n",
        "    elif node_data['label'] == 'tram':\n",
        "        is_stop = 2\n",
        "    elif node_data['label'] == 'subway':\n",
        "        is_stop = 3\n",
        "    features.append([\n",
        "        node_data.get('square', 1),\n",
        "        node_data.get('building_area', 1),\n",
        "        node_data.get('living_area', 1),\n",
        "        node_data.get('population_balanced', 1),\n",
        "        node_data['school'],\n",
        "        node_data['kindergarten'],\n",
        "        node_data['bus_buf'],\n",
        "        node_data['tram_stop'],\n",
        "        node_data['subway_buf'],\n",
        "        node_data['edu_buf'],\n",
        "        node_data['education'],\n",
        "        node_data['food'],\n",
        "        node_data['food_buf'],\n",
        "        is_stop\n",
        "    ])\n",
        "    labels.append(int(node_data['commerce'] > 0))\n",
        "\n",
        "features = torch.tensor(features, dtype=torch.float)\n",
        "num_nodes = features.size(0)\n",
        "labels = torch.tensor(labels, dtype=torch.long)\n",
        "train_indices, test_indices = train_test_split(\n",
        "    np.arange(num_nodes),\n",
        "    test_size=0.3,\n",
        "    stratify=labels.numpy(),\n",
        "    random_state=21\n",
        ")\n",
        "edges = list(map(lambda v: (node_mapping[v[0]], node_mapping[v[1]]), G.edges))\n",
        "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "data = Data(x=features, edge_index=edge_index, y=labels,\n",
        "            train_mask=train_indices, test_mask=test_indices)\n",
        "assert edge_index.max() < num_nodes, \"Edge index is out of bounds!\"\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(N_FEATURES, 32)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(32)\n",
        "        self.conv2 = GCNConv(32, 64)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(64)\n",
        "        self.conv3 = GCNConv(64, 128)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(128)\n",
        "        self.conv4 = GCNConv(128, 256)\n",
        "        self.bn4 = torch.nn.BatchNorm1d(256)\n",
        "        self.conv5 = GCNConv(256, 512)\n",
        "        self.bn5 = torch.nn.BatchNorm1d(512)\n",
        "        self.conv6 = GCNConv(512, 1024)\n",
        "        self.bn6 = torch.nn.BatchNorm1d(1024)\n",
        "        self.conv7 = GCNConv(1024, 2)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv4(x, edge_index)\n",
        "        x = self.bn4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv5(x, edge_index)\n",
        "        x = self.bn5(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv6(x, edge_index)\n",
        "        x = self.bn6(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv7(x, edge_index)\n",
        "        return x\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "model = GCN().to(device)\n",
        "data = data.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "class_weights = torch.tensor([1., 10.])\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "def calc_scores(out_, target_) -> tuple[float, float, float]:\n",
        "    out_max = out_.max(1)[1]\n",
        "    correct = target_.eq(out_max).sum().item()\n",
        "    acc = correct / target_.size()[0]\n",
        "    correct1 = target_[target_.eq(1)].eq(out_max[target_.eq(1)]).sum().item()\n",
        "    acc1 = correct1 / target_[target_.eq(1)].size()[0]\n",
        "    f1 = float(f1_score(target_.numpy(), out_max.numpy(), average='macro'))\n",
        "    return f1, acc, acc1\n",
        "\n",
        "model.train()\n",
        "for epoch in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.edge_index)\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 10 == 0:\n",
        "        model.eval()\n",
        "        out = model(data.x, data.edge_index)\n",
        "        model.train()\n",
        "        f1_train, acc_train, acc1_train = calc_scores(out[data.train_mask], data.y[data.train_mask])\n",
        "        f1_test, acc_test, acc1_test = calc_scores(out[data.test_mask], data.y[data.test_mask])\n",
        "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}, '\n",
        "              f'f1/train: {f1_train:.2f}, acc/avg/train: {acc_train:.2f}, acc/1/train: {acc1_train:.2f}, '\n",
        "              f'f1/test: {f1_test:.2f}, acc/avg/test: {acc_test:.2f}, acc/1/test: {acc1_test:.2f}')\n",
        "\n",
        "def save_to_qgis(data_, pred_):\n",
        "    data_out = []\n",
        "    for i, (node_id, node_data) in enumerate(G.nodes(data=True)):\n",
        "        data_out.append([\n",
        "            node_id,\n",
        "            node_data['label'],\n",
        "            node_data.get('square', 1),\n",
        "            node_data['kindergarten'],\n",
        "            node_data['school'],\n",
        "            node_data['bus_buf'],\n",
        "            node_data['tram_stop'],\n",
        "            node_data['subway_buf'],\n",
        "            node_data['education'],\n",
        "            node_data['edu_buf'],\n",
        "            node_data['food'],\n",
        "            node_data['food_buf'],\n",
        "            node_data['commerce'],\n",
        "            node_data['coordinate'][1],\n",
        "            node_data['coordinate'][0],\n",
        "            pred_[i]\n",
        "        ])\n",
        "    df_out = pd.DataFrame(data_out, columns=['building_id', 'label', 'square', 'kindergarten', 'school', 'bus_buf',\n",
        "                                           'subway_buf', 'tram_stop', 'education', 'edu_buf', 'food', 'food_buf',\n",
        "                                           'commerce', 'lat', 'lon', 'predict'])\n",
        "    print('saving...')\n",
        "    df_out.iloc[data_.train_mask].to_csv('/content/train4.csv', index=False)\n",
        "    df_out.iloc[data_.test_mask].to_csv('/content/test4.csv', index=False)\n",
        "\n",
        "model_filename = 'model3.pth'\n",
        "torch.save(model.state_dict(), model_filename)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred = model(data.x, data.edge_index)\n",
        "    f1, acc_test, acc1_test = calc_scores(pred[data.test_mask], data.y[data.test_mask])\n",
        "    save_to_qgis(data, pred.max(1)[1].numpy())\n",
        "    print(f'TEST: f1: {f1:.2f}, acc/avg/test: {acc_test:.2f}, acc/1/test: {acc1_test:.2f}')"
      ],
      "metadata": {
        "id": "HthxUEqqHEST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code is then generated similarly, requiring only the target variable to be defined and additional input data to be specified.\n",
        "The choice of target depends on the study's goals."
      ],
      "metadata": {
        "id": "NhMA3hYeHEr5"
      }
    }
  ]
}