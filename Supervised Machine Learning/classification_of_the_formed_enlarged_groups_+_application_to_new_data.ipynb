{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost lightgbm xgboost scikit-learn pandas\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
        "import joblib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGhcs7BMWFt8",
        "outputId": "70a457b7-824c-41d2-f0c7-9183403b6c4f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.14.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# education class 1:  personal_development"
      ],
      "metadata": {
        "id": "VIHpEhhFsExH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('personal development edu.csv')\n",
        "\n",
        "df_filtered = df[df['education'] == 1].copy()\n",
        "\n",
        "if len(df_filtered) == 0:\n",
        "    raise ValueError(\"Нет данных с education=1. Проверьте входные данные.\")\n",
        "\n",
        "X = df_filtered.drop(['personal development', 'building_id', 'y', 'x'], axis=1)\n",
        "y = df_filtered['personal development']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=21)\n",
        "\n",
        "param_grids = {\n",
        "    'Random Forest': {\n",
        "        'model': RandomForestClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [3, 5, 7, None],\n",
        "            'min_samples_split': [2, 5],\n",
        "            'class_weight': ['balanced', None]\n",
        "        }\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'model': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'scale_pos_weight': [1, (y_train == 0).sum()/(y_train == 1).sum()]\n",
        "        }\n",
        "    },\n",
        "    'LightGBM': {\n",
        "        'model': LGBMClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'learning_rate': [0.01, 0.1],\n",
        "            'class_weight': ['balanced', None]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "catboost_params = {\n",
        "    'iterations': [50, 100],\n",
        "    'depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'auto_class_weights': ['Balanced', None]\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "\n",
        "for name, config in param_grids.items():\n",
        "    print(f\"\\nПодбор параметров для {name}...\")\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=config['model'],\n",
        "        param_grid=config['params'],\n",
        "        cv=3,\n",
        "        scoring='f1',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    joblib.dump(best_model, f'{name.lower().replace(\" \", \"_\")}_best_model.pkl')\n",
        "\n",
        "    results[name] = {\n",
        "        'best_params': grid_search.best_params_,\n",
        "        'f1_score': f1_score(y_test, y_pred),\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'report': classification_report(y_test, y_pred, zero_division=0)\n",
        "    }\n",
        "\n",
        "    print(f\"Лучшие параметры {name}: {grid_search.best_params_}\")\n",
        "    print(f\"F1-score: {results[name]['f1_score']:.4f}\")\n",
        "    print(f\"Accuracy: {results[name]['accuracy']:.4f}\")\n",
        "\n",
        "\n",
        "print(\"\\nПодбор параметров для CatBoost...\")\n",
        "cat_features = list(X.select_dtypes(include=['object', 'category']).columns)\n",
        "grid_search_cb = GridSearchCV(\n",
        "    estimator=CatBoostClassifier(\n",
        "        random_state=42,\n",
        "        cat_features=cat_features,\n",
        "        verbose=0,\n",
        "        eval_metric='F1'\n",
        "    ),\n",
        "    param_grid=catboost_params,\n",
        "    cv=3,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "grid_search_cb.fit(X_train, y_train)\n",
        "\n",
        "best_cb = grid_search_cb.best_estimator_\n",
        "y_pred_cb = best_cb.predict(X_test)\n",
        "\n",
        "joblib.dump(best_cb, 'catboost_best_model.pkl')\n",
        "\n",
        "results['CatBoost'] = {\n",
        "    'best_params': grid_search_cb.best_params_,\n",
        "    'f1_score': f1_score(y_test, y_pred_cb),\n",
        "    'accuracy': accuracy_score(y_test, y_pred_cb),\n",
        "    'report': classification_report(y_test, y_pred_cb, zero_division=0)\n",
        "}\n",
        "\n",
        "print(\"\\nРезультаты подбора параметров (оптимизация по F1-score):\")\n",
        "for name, res in results.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"Лучшие параметры: {res['best_params']}\")\n",
        "    print(f\"F1-score: {res['f1_score']:.4f}\")\n",
        "    print(f\"Accuracy: {res['accuracy']:.4f}\")\n",
        "    print(\"Отчет классификации:\\n\", res['report'])\n",
        "\n",
        "best_model_name = max(results, key=lambda x: results[x]['f1_score'])\n",
        "best_model = joblib.load(f'{best_model_name.lower().replace(\" \", \"_\")}_best_model.pkl')\n",
        "joblib.dump(best_model, 'best_overall_model.pkl')\n",
        "print(f\"\\nЛучшая модель: {best_model_name} с F1-score {results[best_model_name]['f1_score']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbhUBWPdHcU_",
        "outputId": "1d5206e1-a05c-4909-c1c5-5ba127d4e975",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Подбор параметров для Random Forest...\n",
            "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
            "Лучшие параметры Random Forest: {'class_weight': None, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "F1-score: 0.9394\n",
            "Accuracy: 0.8857\n",
            "\n",
            "Подбор параметров для XGBoost...\n",
            "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [15:19:47] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры XGBoost: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'scale_pos_weight': 1}\n",
            "F1-score: 0.9394\n",
            "Accuracy: 0.8857\n",
            "\n",
            "Подбор параметров для LightGBM...\n",
            "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
            "[LightGBM] [Info] Number of positive: 115, number of negative: 21\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000071 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 53\n",
            "[LightGBM] [Info] Number of data points in the train set: 136, number of used features: 10\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.845588 -> initscore=1.700410\n",
            "[LightGBM] [Info] Start training from score 1.700410\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Лучшие параметры LightGBM: {'class_weight': None, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\n",
            "F1-score: 0.9394\n",
            "Accuracy: 0.8857\n",
            "\n",
            "Подбор параметров для CatBoost...\n",
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
            "\n",
            "Результаты подбора параметров (оптимизация по F1-score):\n",
            "\n",
            "Random Forest:\n",
            "Лучшие параметры: {'class_weight': None, 'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "F1-score: 0.9394\n",
            "Accuracy: 0.8857\n",
            "Отчет классификации:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         4\n",
            "           1       0.89      1.00      0.94        31\n",
            "\n",
            "    accuracy                           0.89        35\n",
            "   macro avg       0.44      0.50      0.47        35\n",
            "weighted avg       0.78      0.89      0.83        35\n",
            "\n",
            "\n",
            "XGBoost:\n",
            "Лучшие параметры: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50, 'scale_pos_weight': 1}\n",
            "F1-score: 0.9394\n",
            "Accuracy: 0.8857\n",
            "Отчет классификации:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         4\n",
            "           1       0.89      1.00      0.94        31\n",
            "\n",
            "    accuracy                           0.89        35\n",
            "   macro avg       0.44      0.50      0.47        35\n",
            "weighted avg       0.78      0.89      0.83        35\n",
            "\n",
            "\n",
            "LightGBM:\n",
            "Лучшие параметры: {'class_weight': None, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\n",
            "F1-score: 0.9394\n",
            "Accuracy: 0.8857\n",
            "Отчет классификации:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         4\n",
            "           1       0.89      1.00      0.94        31\n",
            "\n",
            "    accuracy                           0.89        35\n",
            "   macro avg       0.44      0.50      0.47        35\n",
            "weighted avg       0.78      0.89      0.83        35\n",
            "\n",
            "\n",
            "CatBoost:\n",
            "Лучшие параметры: {'auto_class_weights': 'Balanced', 'depth': 7, 'iterations': 100, 'learning_rate': 0.1}\n",
            "F1-score: 0.8852\n",
            "Accuracy: 0.8000\n",
            "Отчет классификации:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.20      0.25      0.22         4\n",
            "           1       0.90      0.87      0.89        31\n",
            "\n",
            "    accuracy                           0.80        35\n",
            "   macro avg       0.55      0.56      0.55        35\n",
            "weighted avg       0.82      0.80      0.81        35\n",
            "\n",
            "\n",
            "Лучшая модель: Random Forest с F1-score 0.9394\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "36 fits failed out of a total of 72.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "36 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/catboost/core.py\", line 5245, in fit\n",
            "    self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/catboost/core.py\", line 2395, in _fit\n",
            "    train_params = self._prepare_train_params(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/catboost/core.py\", line 2321, in _prepare_train_params\n",
            "    _check_train_params(params)\n",
            "  File \"_catboost.pyx\", line 6601, in _catboost._check_train_params\n",
            "  File \"_catboost.pyx\", line 6623, in _catboost._check_train_params\n",
            "_catboost.CatBoostError: catboost/private/libs/options/json_helper.h:41: Can't parse parameter \"auto_class_weights\" with value: null\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.67572209 0.78006148 0.73227513 0.80162384 0.70651486 0.79806763\n",
            " 0.74374699 0.82332988 0.74922693 0.79649984 0.76300381 0.86310502\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = joblib.load('catboost_best_model.pkl')\n",
        "\n",
        "X_all = df_filtered.drop(['personal development', 'building_id', 'y', 'x'], axis=1)\n",
        "predictions = best_model.predict(X_all)\n",
        "probabilities = best_model.predict_proba(X_all)[:, 1]\n",
        "\n",
        "results_df = df_filtered.copy()\n",
        "\n",
        "results_df['predicted_class'] = predictions\n",
        "results_df['prediction_probability'] = probabilities\n",
        "\n",
        "results_df.to_csv('qgis_full_features_export.csv', index=False)\n",
        "print(\"Результаты со всеми признаками сохранены в qgis_full_features_export.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AS5Okc5T5A7_",
        "outputId": "4862287d-d55d-43d9-e182-ad2e413dac67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результаты со всеми признаками сохранены в qgis_full_features_export.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "application to new data"
      ],
      "metadata": {
        "id": "11a06OkvtNV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = pd.read_csv('data_high14.csv')\n",
        "\n",
        "new_data_filtered = new_data[new_data['education'] == 1].copy()\n",
        "\n",
        "best_model = joblib.load('catboost_best_model.pkl')\n",
        "\n",
        "cols_to_drop = ['personal development', 'building_id', 'y', 'x']\n",
        "X_new = new_data_filtered.drop([col for col in cols_to_drop if col in new_data_filtered.columns], axis=1)\n",
        "\n",
        "predictions = best_model.predict(X_new)\n",
        "probabilities = best_model.predict_proba(X_new)[:, 1]\n",
        "\n",
        "full_results = new_data.copy()\n",
        "\n",
        "full_results['predicted_class'] = 0\n",
        "full_results['prediction_probability'] = 0\n",
        "full_results.loc[new_data['education'] == 1, 'predicted_class'] = predictions\n",
        "full_results.loc[new_data['education'] == 1, 'prediction_probability'] = probabilities\n",
        "\n",
        "full_results.to_csv('full_predictions_with_all_data.csv', index=False)\n",
        "print(\"Полные результаты сохранены в full_predictions_with_all_data.csv\")\n",
        "\n",
        "if {'y', 'x'}.issubset(full_results.columns):\n",
        "    qgis_cols = ['building_id', 'y', 'x', 'predicted_class', 'prediction_probability'] + \\\n",
        "                [col for col in new_data.columns if col not in cols_to_drop]\n",
        "    full_results[qgis_cols].to_csv('qgis_full_export.csv', index=False)\n",
        "    print(\"Данные для QGIS сохранены в qgis_full_export.csv\")"
      ],
      "metadata": {
        "id": "UC9kayB3VHTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  education class 2: else_edu"
      ],
      "metadata": {
        "id": "nCniAUKUunYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('else_edu.csv')\n",
        "\n",
        "df_filtered = df[df['education'] == 1].copy()\n",
        "\n",
        "if len(df_filtered) == 0:\n",
        "    raise ValueError(\"Нет данных с education=1. Проверьте входные данные.\")\n",
        "\n",
        "X = df_filtered.drop(['else', 'building_id', 'y', 'x'], axis=1)\n",
        "y = df_filtered['else']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "joblib.dump(X_train.columns.tolist(), 'train_features.pkl')\n",
        "\n",
        "param_grids = {\n",
        "    'Random Forest': {\n",
        "        'model': RandomForestClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [3, 5, 7, None],\n",
        "            'min_samples_split': [2, 5],\n",
        "            'class_weight': ['balanced', None]\n",
        "        }\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'model': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'scale_pos_weight': [1, (y_train == 0).sum()/(y_train == 1).sum()]\n",
        "        }\n",
        "    },\n",
        "    'LightGBM': {\n",
        "        'model': LGBMClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'learning_rate': [0.01, 0.1],\n",
        "            'class_weight': ['balanced', None]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "catboost_params = {\n",
        "    'iterations': [50, 100],\n",
        "    'depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'auto_class_weights': ['Balanced', None]\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, config in param_grids.items():\n",
        "    print(f\"\\nПодбор параметров для {name}...\")\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=config['model'],\n",
        "        param_grid=config['params'],\n",
        "        cv=3,\n",
        "        scoring='f1',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    joblib.dump(best_model, f'{name.lower().replace(\" \", \"_\")}_best_model1.pkl')\n",
        "\n",
        "    results[name] = {\n",
        "        'best_params': grid_search.best_params_,\n",
        "        'f1_score': f1_score(y_test, y_pred),\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'report': classification_report(y_test, y_pred, zero_division=0)\n",
        "    }\n",
        "\n",
        "    print(f\"Лучшие параметры {name}: {grid_search.best_params_}\")\n",
        "    print(f\"F1-score: {results[name]['f1_score']:.4f}\")\n",
        "    print(f\"Accuracy: {results[name]['accuracy']:.4f}\")\n",
        "\n",
        "print(\"\\nПодбор параметров для CatBoost...\")\n",
        "cat_features = list(X.select_dtypes(include=['object', 'category']).columns)\n",
        "grid_search_cb = GridSearchCV(\n",
        "    estimator=CatBoostClassifier(\n",
        "        random_state=42,\n",
        "        cat_features=cat_features,\n",
        "        verbose=0,\n",
        "        eval_metric='F1'\n",
        "    ),\n",
        "    param_grid=catboost_params,\n",
        "    cv=3,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "grid_search_cb.fit(X_train, y_train)\n",
        "\n",
        "best_cb = grid_search_cb.best_estimator_\n",
        "y_pred_cb = best_cb.predict(X_test)\n",
        "\n",
        "joblib.dump(best_cb, 'catboost_best_model1.pkl')\n",
        "\n",
        "results['CatBoost'] = {\n",
        "    'best_params': grid_search_cb.best_params_,\n",
        "    'f1_score': f1_score(y_test, y_pred_cb),\n",
        "    'accuracy': accuracy_score(y_test, y_pred_cb),\n",
        "    'report': classification_report(y_test, y_pred_cb, zero_division=0)\n",
        "}\n",
        "\n",
        "print(\"\\nРезультаты подбора параметров (оптимизация по F1-score):\")\n",
        "for name, res in results.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"Лучшие параметры: {res['best_params']}\")\n",
        "    print(f\"F1-score: {res['f1_score']:.4f}\")\n",
        "    print(f\"Accuracy: {res['accuracy']:.4f}\")\n",
        "    print(\"Отчет классификации:\\n\", res['report'])\n",
        "\n",
        "best_model_name = max(results, key=lambda x: results[x]['f1_score'])\n",
        "best_model = joblib.load(f'{best_model_name.lower().replace(\" \", \"_\")}_best_model1.pkl')\n",
        "joblib.dump(best_model, 'best_overall_model1.pkl')\n",
        "print(f\"\\nЛучшая модель: {best_model_name} с F1-score {results[best_model_name]['f1_score']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fJL-KnQGuekT",
        "outputId": "efee7429-8ea3-4eed-9b18-3e64c858804f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Подбор параметров для Random Forest...\n",
            "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
            "Лучшие параметры Random Forest: {'class_weight': None, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "F1-score: 0.9091\n",
            "Accuracy: 0.9714\n",
            "\n",
            "Подбор параметров для XGBoost...\n",
            "Fitting 3 folds for each of 54 candidates, totalling 162 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [20:13:55] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лучшие параметры XGBoost: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'scale_pos_weight': 1}\n",
            "F1-score: 0.9091\n",
            "Accuracy: 0.9714\n",
            "\n",
            "Подбор параметров для LightGBM...\n",
            "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 27, number of negative: 109\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000082 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 57\n",
            "[LightGBM] [Info] Number of data points in the train set: 136, number of used features: 11\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
            "[LightGBM] [Info] Start training from score -0.000000\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Лучшие параметры LightGBM: {'class_weight': 'balanced', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\n",
            "F1-score: 0.9091\n",
            "Accuracy: 0.9714\n",
            "\n",
            "Подбор параметров для CatBoost...\n",
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
            "\n",
            "Результаты подбора параметров (оптимизация по F1-score):\n",
            "\n",
            "Random Forest:\n",
            "Лучшие параметры: {'class_weight': None, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 200}\n",
            "F1-score: 0.9091\n",
            "Accuracy: 0.9714\n",
            "Отчет классификации:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98        29\n",
            "           1       1.00      0.83      0.91         6\n",
            "\n",
            "    accuracy                           0.97        35\n",
            "   macro avg       0.98      0.92      0.95        35\n",
            "weighted avg       0.97      0.97      0.97        35\n",
            "\n",
            "\n",
            "XGBoost:\n",
            "Лучшие параметры: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 100, 'scale_pos_weight': 1}\n",
            "F1-score: 0.9091\n",
            "Accuracy: 0.9714\n",
            "Отчет классификации:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98        29\n",
            "           1       1.00      0.83      0.91         6\n",
            "\n",
            "    accuracy                           0.97        35\n",
            "   macro avg       0.98      0.92      0.95        35\n",
            "weighted avg       0.97      0.97      0.97        35\n",
            "\n",
            "\n",
            "LightGBM:\n",
            "Лучшие параметры: {'class_weight': 'balanced', 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\n",
            "F1-score: 0.9091\n",
            "Accuracy: 0.9714\n",
            "Отчет классификации:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98        29\n",
            "           1       1.00      0.83      0.91         6\n",
            "\n",
            "    accuracy                           0.97        35\n",
            "   macro avg       0.98      0.92      0.95        35\n",
            "weighted avg       0.97      0.97      0.97        35\n",
            "\n",
            "\n",
            "CatBoost:\n",
            "Лучшие параметры: {'auto_class_weights': 'Balanced', 'depth': 3, 'iterations': 50, 'learning_rate': 0.01}\n",
            "F1-score: 0.9091\n",
            "Accuracy: 0.9714\n",
            "Отчет классификации:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.98        29\n",
            "           1       1.00      0.83      0.91         6\n",
            "\n",
            "    accuracy                           0.97        35\n",
            "   macro avg       0.98      0.92      0.95        35\n",
            "weighted avg       0.97      0.97      0.97        35\n",
            "\n",
            "\n",
            "Лучшая модель: Random Forest с F1-score 0.9091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "36 fits failed out of a total of 72.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "36 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/catboost/core.py\", line 5245, in fit\n",
            "    self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/catboost/core.py\", line 2395, in _fit\n",
            "    train_params = self._prepare_train_params(\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/catboost/core.py\", line 2321, in _prepare_train_params\n",
            "    _check_train_params(params)\n",
            "  File \"_catboost.pyx\", line 6601, in _catboost._check_train_params\n",
            "  File \"_catboost.pyx\", line 6623, in _catboost._check_train_params\n",
            "_catboost.CatBoostError: catboost/private/libs/options/json_helper.h:41: Can't parse parameter \"auto_class_weights\" with value: null\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.85       0.85       0.85       0.73148148 0.85       0.83284314\n",
            " 0.85       0.74016741 0.85       0.85       0.85       0.76851852\n",
            "        nan        nan        nan        nan        nan        nan\n",
            "        nan        nan        nan        nan        nan        nan]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = joblib.load('best_overall_model1.pkl')\n",
        "\n",
        "X_all = df_filtered.drop(['else', 'building_id', 'y', 'x'], axis=1)\n",
        "predictions = best_model.predict(X_all)\n",
        "probabilities = best_model.predict_proba(X_all)[:, 1]\n",
        "\n",
        "results_df = df_filtered.copy()\n",
        "\n",
        "results_df['predicted_class'] = predictions\n",
        "results_df['prediction_probability'] = probabilities\n",
        "\n",
        "results_df.to_csv('qgis_full_features_export1.csv', index=False)\n",
        "print(\"Результаты со всеми признаками сохранены в qgis_full_features_export1.csv\")\n",
        "\n",
        "print(\"\\nПроверка содержимого CSV:\")\n",
        "print(pd.read_csv('qgis_full_features_export1.csv').columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pp8yNzqxxrmf",
        "outputId": "420b9f29-8f09-401f-b651-32534d96a6bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результаты со всеми признаками сохранены в qgis_full_features_export1.csv\n",
            "\n",
            "Проверка содержимого CSV:\n",
            "Index(['personal_development', 'else', 'education', 'food', 'commerce',\n",
            "       'finance', 'health', 'recreation', 'kindergarten', 'school', 'edu_buf',\n",
            "       'food_buf', 'fin_buf', 'commerce_buf', 'health_buf', 'subway_buf',\n",
            "       'bus_buf', 'tram_stop', 'building_id', 'y', 'x', 'predicted_class',\n",
            "       'prediction_probability'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "application to new data"
      ],
      "metadata": {
        "id": "zjNC3zmqxsSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = pd.read_csv('data_high15.csv')\n",
        "\n",
        "new_data_filtered = new_data[new_data['education'] == 1].copy()\n",
        "\n",
        "best_model = joblib.load('best_overall_model1.pkl')\n",
        "\n",
        "cols_to_drop = ['else', 'building_id', 'y', 'x']\n",
        "X_new = new_data_filtered.drop([col for col in cols_to_drop if col in new_data_filtered.columns], axis=1)\n",
        "\n",
        "predictions = best_model.predict(X_new)\n",
        "probabilities = best_model.predict_proba(X_new)[:, 1]\n",
        "\n",
        "full_results = new_data.copy()\n",
        "\n",
        "full_results['predicted_class'] = 0\n",
        "full_results['prediction_probability'] = 0\n",
        "full_results.loc[new_data['education'] == 1, 'predicted_class'] = predictions\n",
        "full_results.loc[new_data['education'] == 1, 'prediction_probability'] = probabilities\n",
        "\n",
        "full_results.to_csv('full_predictions_with_all_data1.csv', index=False)\n",
        "print(\"Полные результаты сохранены в full_predictions_with_all_data1.csv\")\n",
        "\n",
        "if {'y', 'x'}.issubset(full_results.columns):\n",
        "    qgis_cols = ['building_id', 'y', 'x', 'predicted_class', 'prediction_probability'] + \\\n",
        "                [col for col in new_data.columns if col not in cols_to_drop]\n",
        "    full_results[qgis_cols].to_csv('qgis_full_export1.csv', index=False)\n",
        "    print(\"Данные для QGIS сохранены в qgis_full_export1.csv\")"
      ],
      "metadata": {
        "id": "TAFm3-LUJiHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Food|eat"
      ],
      "metadata": {
        "id": "E9GJiqWeg20z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('eat.csv')\n",
        "\n",
        "df_filtered = df[df['food'] == 1].copy()\n",
        "\n",
        "if len(df_filtered) == 0:\n",
        "    raise ValueError(\"Нет данных с food=1. Проверьте входные данные.\")\n",
        "\n",
        "X = df_filtered.drop(['eat', 'building_id', 'y', 'x'], axis=1)\n",
        "y = df_filtered['eat']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "joblib.dump(X_train.columns.tolist(), 'train_features_eat.pkl')\n",
        "\n",
        "param_grids = {\n",
        "    'Random Forest': {\n",
        "        'model': RandomForestClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [3, 5, 7, None],\n",
        "            'min_samples_split': [2, 5],\n",
        "            'class_weight': ['balanced', None]\n",
        "        }\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'model': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'scale_pos_weight': [1, (y_train == 0).sum()/(y_train == 1).sum()]\n",
        "        }\n",
        "    },\n",
        "    'LightGBM': {\n",
        "        'model': LGBMClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'learning_rate': [0.01, 0.1],\n",
        "            'class_weight': ['balanced', None]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "catboost_params = {\n",
        "    'iterations': [50, 100],\n",
        "    'depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'auto_class_weights': ['Balanced', None]\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, config in param_grids.items():\n",
        "    print(f\"\\nПодбор параметров для {name}...\")\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=config['model'],\n",
        "        param_grid=config['params'],\n",
        "        cv=3,\n",
        "        scoring='f1',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    joblib.dump(best_model, f'{name.lower().replace(\" \", \"_\")}_best_model_eat.pkl')\n",
        "\n",
        "    results[name] = {\n",
        "        'best_params': grid_search.best_params_,\n",
        "        'f1_score': f1_score(y_test, y_pred),\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'report': classification_report(y_test, y_pred, zero_division=0)\n",
        "    }\n",
        "\n",
        "    print(f\"Лучшие параметры {name}: {grid_search.best_params_}\")\n",
        "    print(f\"F1-score: {results[name]['f1_score']:.4f}\")\n",
        "    print(f\"Accuracy: {results[name]['accuracy']:.4f}\")\n",
        "\n",
        "print(\"\\nПодбор параметров для CatBoost...\")\n",
        "cat_features = list(X.select_dtypes(include=['object', 'category']).columns)\n",
        "grid_search_cb = GridSearchCV(\n",
        "    estimator=CatBoostClassifier(\n",
        "        random_state=42,\n",
        "        cat_features=cat_features,\n",
        "        verbose=0,\n",
        "        eval_metric='F1'\n",
        "    ),\n",
        "    param_grid=catboost_params,\n",
        "    cv=3,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "grid_search_cb.fit(X_train, y_train)\n",
        "\n",
        "best_cb = grid_search_cb.best_estimator_\n",
        "y_pred_cb = best_cb.predict(X_test)\n",
        "\n",
        "joblib.dump(best_cb, 'catboost_best_model_eat.pkl')\n",
        "\n",
        "results['CatBoost'] = {\n",
        "    'best_params': grid_search_cb.best_params_,\n",
        "    'f1_score': f1_score(y_test, y_pred_cb),\n",
        "    'accuracy': accuracy_score(y_test, y_pred_cb),\n",
        "    'report': classification_report(y_test, y_pred_cb, zero_division=0)\n",
        "}\n",
        "\n",
        "print(\"\\nРезультаты подбора параметров (оптимизация по F1-score):\")\n",
        "for name, res in results.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"Лучшие параметры: {res['best_params']}\")\n",
        "    print(f\"F1-score: {res['f1_score']:.4f}\")\n",
        "    print(f\"Accuracy: {res['accuracy']:.4f}\")\n",
        "    print(\"Отчет классификации:\\n\", res['report'])\n",
        "\n",
        "best_model_name = max(results, key=lambda x: results[x]['f1_score'])\n",
        "best_model = joblib.load(f'{best_model_name.lower().replace(\" \", \"_\")}_best_model_eat.pkl')\n",
        "joblib.dump(best_model, 'best_overall_model_eat.pkl')\n",
        "print(f\"\\nЛучшая модель: {best_model_name} с F1-score {results[best_model_name]['f1_score']:.4f}\")"
      ],
      "metadata": {
        "id": "abgHQEkYg7Bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = joblib.load('best_overall_model_eat.pkl')\n",
        "\n",
        "X_all = df_filtered.drop(['eat', 'building_id', 'y', 'x'], axis=1)\n",
        "predictions = best_model.predict(X_all)\n",
        "probabilities = best_model.predict_proba(X_all)[:, 1]\n",
        "\n",
        "results_df = df_filtered.copy()\n",
        "\n",
        "results_df['predicted_class'] = predictions\n",
        "results_df['prediction_probability'] = probabilities\n",
        "\n",
        "results_df.to_csv('qgis_full_features_eat.csv', index=False)\n",
        "print(\"Результаты со всеми признаками сохранены в qgis_full_features_eat.csv\")\n",
        "\n",
        "print(\"\\nПроверка содержимого CSV:\")\n",
        "print(pd.read_csv('qgis_full_features_eat.csv').columns)"
      ],
      "metadata": {
        "id": "ouBGn4z6XQbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "application to new data"
      ],
      "metadata": {
        "id": "N1axyIJOXdVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = pd.read_csv('data_high16.csv')\n",
        "\n",
        "new_data_filtered = new_data[new_data['food'] == 1].copy()\n",
        "\n",
        "best_model = joblib.load('best_overall_model_eat.pkl')\n",
        "\n",
        "cols_to_drop = ['eat', 'building_id', 'y', 'x']\n",
        "X_new = new_data_filtered.drop([col for col in cols_to_drop if col in new_data_filtered.columns], axis=1)\n",
        "\n",
        "predictions = best_model.predict(X_new)\n",
        "probabilities = best_model.predict_proba(X_new)[:, 1]\n",
        "\n",
        "full_results = new_data.copy()\n",
        "\n",
        "full_results['predicted_class'] = 0\n",
        "full_results['prediction_probability'] = 0\n",
        "full_results.loc[new_data['education'] == 1, 'predicted_class'] = predictions\n",
        "full_results.loc[new_data['education'] == 1, 'prediction_probability'] = probabilities\n",
        "\n",
        "full_results.to_csv('full_predictions_with_all_data_eat.csv', index=False)\n",
        "print(\"Полные результаты сохранены в full_predictions_with_all_data_eat.csv\")\n",
        "\n",
        "if {'y', 'x'}.issubset(full_results.columns):\n",
        "    qgis_cols = ['building_id', 'y', 'x', 'predicted_class', 'prediction_probability'] + \\\n",
        "                [col for col in new_data.columns if col not in cols_to_drop]\n",
        "    full_results[qgis_cols].to_csv('qgis_full_export_eat.csv', index=False)\n",
        "    print(\"Данные для QGIS сохранены в qgis_full_export_eat.csv\")"
      ],
      "metadata": {
        "id": "BHmtR2RqXdyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Food|drink"
      ],
      "metadata": {
        "id": "XibUiMTbg7ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('drink.csv')\n",
        "\n",
        "df_filtered = df[df['food'] == 1].copy()\n",
        "\n",
        "if len(df_filtered) == 0:\n",
        "    raise ValueError(\"Нет данных с food=1. Проверьте входные данные.\")\n",
        "\n",
        "X = df_filtered.drop(['drink', 'building_id', 'y', 'x'], axis=1)\n",
        "y = df_filtered['drink']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "joblib.dump(X_train.columns.tolist(), 'train_features_drink.pkl')\n",
        "\n",
        "param_grids = {\n",
        "    'Random Forest': {\n",
        "        'model': RandomForestClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [3, 5, 7, None],\n",
        "            'min_samples_split': [2, 5],\n",
        "            'class_weight': ['balanced', None]\n",
        "        }\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'model': XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'learning_rate': [0.01, 0.1, 0.2],\n",
        "            'scale_pos_weight': [1, (y_train == 0).sum()/(y_train == 1).sum()]\n",
        "        }\n",
        "    },\n",
        "    'LightGBM': {\n",
        "        'model': LGBMClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'learning_rate': [0.01, 0.1],\n",
        "            'class_weight': ['balanced', None]\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "catboost_params = {\n",
        "    'iterations': [50, 100],\n",
        "    'depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.1],\n",
        "    'auto_class_weights': ['Balanced', None]\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, config in param_grids.items():\n",
        "    print(f\"\\nПодбор параметров для {name}...\")\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=config['model'],\n",
        "        param_grid=config['params'],\n",
        "        cv=3,\n",
        "        scoring='f1',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "    y_pred = best_model.predict(X_test)\n",
        "\n",
        "    joblib.dump(best_model, f'{name.lower().replace(\" \", \"_\")}_best_model_drink.pkl')\n",
        "\n",
        "    results[name] = {\n",
        "        'best_params': grid_search.best_params_,\n",
        "        'f1_score': f1_score(y_test, y_pred),\n",
        "        'accuracy': accuracy_score(y_test, y_pred),\n",
        "        'report': classification_report(y_test, y_pred, zero_division=0)\n",
        "    }\n",
        "\n",
        "    print(f\"Лучшие параметры {name}: {grid_search.best_params_}\")\n",
        "    print(f\"F1-score: {results[name]['f1_score']:.4f}\")\n",
        "    print(f\"Accuracy: {results[name]['accuracy']:.4f}\")\n",
        "\n",
        "print(\"\\nПодбор параметров для CatBoost...\")\n",
        "cat_features = list(X.select_dtypes(include=['object', 'category']).columns)\n",
        "grid_search_cb = GridSearchCV(\n",
        "    estimator=CatBoostClassifier(\n",
        "        random_state=42,\n",
        "        cat_features=cat_features,\n",
        "        verbose=0,\n",
        "        eval_metric='F1'\n",
        "    ),\n",
        "    param_grid=catboost_params,\n",
        "    cv=3,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "grid_search_cb.fit(X_train, y_train)\n",
        "\n",
        "best_cb = grid_search_cb.best_estimator_\n",
        "y_pred_cb = best_cb.predict(X_test)\n",
        "\n",
        "joblib.dump(best_cb, 'catboost_best_model_drink.pkl')\n",
        "\n",
        "results['CatBoost'] = {\n",
        "    'best_params': grid_search_cb.best_params_,\n",
        "    'f1_score': f1_score(y_test, y_pred_cb),\n",
        "    'accuracy': accuracy_score(y_test, y_pred_cb),\n",
        "    'report': classification_report(y_test, y_pred_cb, zero_division=0)\n",
        "}\n",
        "\n",
        "print(\"\\nРезультаты подбора параметров (оптимизация по F1-score):\")\n",
        "for name, res in results.items():\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"Лучшие параметры: {res['best_params']}\")\n",
        "    print(f\"F1-score: {res['f1_score']:.4f}\")\n",
        "    print(f\"Accuracy: {res['accuracy']:.4f}\")\n",
        "    print(\"Отчет классификации:\\n\", res['report'])\n",
        "\n",
        "best_model_name = max(results, key=lambda x: results[x]['f1_score'])\n",
        "best_model = joblib.load(f'{best_model_name.lower().replace(\" \", \"_\")}_best_model_drink.pkl')\n",
        "joblib.dump(best_model, 'best_overall_model_drink.pkl')\n",
        "print(f\"\\nЛучшая модель: {best_model_name} с F1-score {results[best_model_name]['f1_score']:.4f}\")"
      ],
      "metadata": {
        "id": "eI8lbc9ZW1Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = joblib.load('best_overall_model_drink.pkl')\n",
        "\n",
        "X_all = df_filtered.drop(['eat', 'building_id', 'y', 'x'], axis=1)\n",
        "predictions = best_model.predict(X_all)\n",
        "probabilities = best_model.predict_proba(X_all)[:, 1]\n",
        "\n",
        "results_df = df_filtered.copy()\n",
        "\n",
        "results_df['predicted_class'] = predictions\n",
        "results_df['prediction_probability'] = probabilities\n",
        "\n",
        "results_df.to_csv('qgis_full_features_drink.csv', index=False)\n",
        "print(\"Результаты со всеми признаками сохранены в qgis_full_features_drink.csv\")\n",
        "\n",
        "print(\"\\nПроверка содержимого CSV:\")\n",
        "print(pd.read_csv('qgis_full_features_drink.csv').columns)"
      ],
      "metadata": {
        "id": "pKWDc_5ZXW6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "application to new data"
      ],
      "metadata": {
        "id": "0ma6wdrLXq8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_data = pd.read_csv('data_high17.csv')\n",
        "\n",
        "new_data_filtered = new_data[new_data['food'] == 1].copy()\n",
        "\n",
        "best_model = joblib.load('best_overall_model_drink.pkl')\n",
        "\n",
        "cols_to_drop = ['drink', 'building_id', 'y', 'x']\n",
        "X_new = new_data_filtered.drop([col for col in cols_to_drop if col in new_data_filtered.columns], axis=1)\n",
        "\n",
        "predictions = best_model.predict(X_new)\n",
        "probabilities = best_model.predict_proba(X_new)[:, 1]\n",
        "\n",
        "full_results = new_data.copy()\n",
        "\n",
        "full_results['predicted_class'] = 0\n",
        "full_results['prediction_probability'] = 0\n",
        "full_results.loc[new_data['education'] == 1, 'predicted_class'] = predictions\n",
        "full_results.loc[new_data['education'] == 1, 'prediction_probability'] = probabilities\n",
        "\n",
        "full_results.to_csv('full_predictions_with_all_data_drink.csv', index=False)\n",
        "print(\"Полные результаты сохранены в full_predictions_with_all_data_drink.csv\")\n",
        "\n",
        "if {'y', 'x'}.issubset(full_results.columns):\n",
        "    qgis_cols = ['building_id', 'y', 'x', 'predicted_class', 'prediction_probability'] + \\\n",
        "                [col for col in new_data.columns if col not in cols_to_drop]\n",
        "    full_results[qgis_cols].to_csv('qgis_full_export_drink.csv', index=False)\n",
        "    print(\"Данные для QGIS сохранены в qgis_full_export_drink.csv\")"
      ],
      "metadata": {
        "id": "zbI544F3XqS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code is then generated similarly, requiring only the target variable to be defined and additional input data to be specified. The choice of target depends on the study's goals."
      ],
      "metadata": {
        "id": "Ua9_TDe2X3-v"
      }
    }
  ]
}